\label{Chapter:SimCLR}
Until this point of the work, we have been presenting the theoretical basis of representation learning using contrastive learning. Previous approaches, such as  the framework presented in \cite{oord_representation_2019}, use a generative approach as a part of the representation learning process. Although this can be benefitial at some points and, in fact, achieved the \emph{state-of-art}\footnotemark empirical results, we have to consider that generative models have some drawbacks. 

%------------- Footnotemark
\footnotetext{\emph{State-of-art} reffers to the best results that have been achieved at some point of time.}
%----------------------


Let us set in the case of learning representations of images to present a very simple example. In this case, generative models must \emph{generate} each pixel on the image. This can be extremely computationally expensive. 

Until now, we had been trying to minimize the loss in Equation \eqref{NCE:loss}, which we proved that maximizes a lower bound in the mutual information. However, some papers such as \cite{chen_simple_2020}, \cite{tschannen_mutual_2020}, suggest that it is unclear if the success of their methods is caused by the maximization of mutual information between the latent representations, or by the specific form that the constrastive loss has.

In fact, in \cite{tschannen_mutual_2020} they provide empirical proof for the loose connection between the success of the methods that use MI maximization and the utility of the MI maximization in practice. They also empirically proof  that the encoder architecture can be more important than the estimator used to determine the MI.

Even with the empirically proved disconnection between MI maximization and representation quality, recent works that have used the loss function \eqref{NCE:loss} have obtained state-of-art results in practice. 

\section{SimCLR}

\emph{SimCLR} \citep{chen_simple_2020} presents a framework that achieved state-of-art results when it was presented in July 2020. It also makes use of contrastive learning, focusing in the study of the following findings:
\begin{itemize}
\item In Machine Learning, \emph{data augmentation} reffers to the process of artificially creating new examples of the data by applying transformations to the data that we already have available. In the SimCLR framework, multiple data augmentations are used and it is empirically shown that this improves the contrastive prediction tasks that yield effective representations.

\item The introduction of a nonlinear transformation that it can be learnt during the learning process. This linear transformation is between the representation and the contrastive loss, so before evaluating the loss function, the representation is applied the nonlinear function.

\item The contrastive loss benefits from normalized embeddings and also from a temperature parameter that has to be adjusted.

\item This loss also benefits from larger batch sizes and longer training, as well as from deeper and wider networks.
\end{itemize}

This framework learns representations by maximizing agreement between examples of the same input obtained by using data augmentation on tha input example and a contrastive loss in the latent space.