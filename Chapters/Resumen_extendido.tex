En este trabajo, trataremos de exponer cómo ha ido evolucionando el \emph{aprendizaje de representaciones} desde los primeros métodos usados hasta los nuevos marcos de trabajo usados en este campo. En este trabajo, realizaremos un estudio \textbf{teórico} de los conceptos necesarios para aproximarnos al problema de aprendizaje de representaciones (partes 1 y 2) y, a continuación y de forma algo más \textbf{práctica}, presentaremos algunos marcos de trabajo utilizados y realizaremos una serie de experimentos utilizando estos marcos (partes 3 y 4).

La parte \textbf{teórica} describe y profundiza los conceptos más importantes y en los que se fundamenta la parte prática.

En la \textbf{primera parte} se comienza haciendo una introducción y motivación profunda al tema (capítulo 1), y se sigue en un extenso capítulo (capítulo 2) en el que se describen todos los conceptos fundamentales para comprender este trabajo. Se dan primero las nociones más básicas sobre probabilidad como el de variable aleatoria, su esperanza y el concepto de {divergencia de Kullback-Leibler}, que será muy relevante pues será una de las formas que tengamos de expresar la \emph{Información Mutua}. Seguidamente, se dan algunas nociones algo más complejas sobre inferencia estadística, como el de función de verosimilitud y los modelos generativos. Se termina este capítulo realizando una  introdución al aprendizaje profundo  resaltando el \emph{aumento de datos}, que proporciona nuevos ejemplos a partir de los datos obtenidos y que juega un papel crucial en el aprendizaje de representaciones.

A continuación (capítulo 3), nos dedicamos a presentar  los conceptos más importantes de la \emph{teoría de la información} en los que se basa nuestro trabajo. Primeramente se expone el concepto de  \emph{entropía} y se dan ciertas propiedades de la misma. Estas propiedades son útiles pues cuando definimos la \emph{información mutua}, al definirse esta en función de la entropía, se pueden extrapolar sin ningún esfuerzo a propiedades de la información mutua. Se introducen además dos de las tres principales \emph{cotas inferiores} para la información mutua: la \emph{cota inferior variacional} y la cota inferior usando la\emph{representación Donsker-Varadhan} de divergencia de Kullback-Leibler. Estas cotas son muy útiles a la hora de hacer aproximaciones al valor real de la información mutua.


La \textbf{segunda parte} está dedicada a estudiar de forma profunda el problema del aprendizaje contrastivo, cómo surgió y cómo ha evolucionado. Es necesario para esto comenzar (capítulo 4) dando una descripción detallada de cómo se ajusta un modelo de regresión logística, pues se utiliza para resolver el problema de la \emph{estimación de ruido contrastiva}, teoría en la que basamos el aprendizaje contrastivo. Más adelante (capítulo 5), se introduce el marco de trabajo del aprendizaje contrastivo en el que, fijado un conjunto de datos, se trata de discriminar entre datos obtenidos de dos distribuciones de probabilidad $P$ y $Q$ diferentes. Para ello, se utiliza la función de pérdida contrastiva, que se demostrará empíricamente que es clave en el aprendizaje de representaciones. Además, se demuestra la última cota inferior que daremos para la información mutua, la \emph{cota contrastiva}. Por último en esta parte, se introducen las funciones de pérdida utilizando tripletas (capítulo 6), que son una generalización de la función de pérdida contrastiva y se demuestra cómo obtener la función de pérdida contrastiva a partir de una función de pérdida usando tripletas.

En la \textbf{parte práctica}, se explican los principales marcos de trabajo que se utilizan y se exponen los experimentos realizados y resultados obtenidos.

La \textbf{tercera parte} presenta dos redes siamesas:\emph{SimCLR} (capítulo 7) y \emph{Bootstrap your own latent (BYOL)} (capítulo 8), los dos marcos de trabajo que han surgido en el año 2020 para el aprendizaje de representaciones. En ambos casos, se da una motivación de por qué surgen, se explica la arquitectura que ambas siguen y las funciones de pérdida que utilizan cada una, y se comenta qué hiperparámetros pueden ser más relevantes a la hora de entrenar los modelos y obtener mejores representaciones para las tareas posteriores como clasificación o regresión.

En la \textbf{cuarta y última parte} se exponen los experimentos realizados utilizando los marcos anteriores. Primeramente (capítulo 9), se exponen los objetivos que se persiguen mediante estos experimentos, que se resumen en adaptar los experimentos existentes a los recursos de los que se disponen. Además, se exponen las tecnologías utilizadas. Seguidamente (capítulo 10), se exponen primero los tres experimentos realizados utilizando SimCLR, un primero general, un segundo aumentando el tamaño y profundidad del \emph{encoder} del marco, y un tercero añadiendo una nueva capa de aumento de datos y preprocesamiento de los mismos. Los experimentos resultan exitosos, obteniendo en el tercero mejoras respecto al primero y resultados acordes a lo previsto. Lo mismo oscurre más tarde cuando realizamos dos experimentos utilizando BYOL, un primero en el que se demuestra empíricamente que la influencia que tenía el \emph{tamaño del batch} en SimCLR se pierde en BYOL, y un segundo en el que se amplía de nuevo el tamaño del encoder, obteniendo también resultados exitosos.

\textbf{Palabras clave:} \emph{información mutua}, \emph{estimación contrastiva de ruido}, \emph{entropía}, \emph{aprendizaje de representaciones}, \emph{redes siamesas}, \emph{pérdida usando tripletas}, \emph{aprendizaje profundo}, \emph{cotas inferiores}.
