Obtaining good representations of data is one of the most important tasks in Machine learning. Good features of our data will lead us to easier and more accurate training on \emph{Artificial Neural Networks (ANNs)} and, thus, better results on experiments.

Recently, it has been discovered that maximizing \emph{Mutual Information} between two elements in our data can give us good representations for our data. We will go through the basic concepts first.


\section{Entropy}

The \emph{mutual information} concept is based on the \emph{Shannon entropy}, which we will introduce first, along with some basic properties of it. The \emph{Shannon entropy} its a way of measuring the uncertainty in a random variable. Given an event $\mathcal A \in \Omega$, $\Prob$ a probability measure and $\Prob[\A]$ the probability of $\mathcal A$, we can affirm that 
$$
\log\frac{1}{\Prob[\mathcal A]}
$$
describes \emph{how surprising is that $\A$ occurs}. For instance, if $P[\A] = 1$, then the last expression is zero, which means that it is not a surprise that $\A$ occurred. With this motivation, we get to the following definition.

\subsection{Discrete case}

\begin{ndef}
Let $X$ be a discrete random variable with image $\X$. The \emph{Shannon entropy}, or simply \emph{entropy} , $H(X)$ of $X$ is defined as:
$$
H(X) = E_X\left[\log\frac{1}{\Prob_X(X)}\right] =  \sum_{x \in \X} P_X(x) \log\frac{1}{\Prob_X(x)}
$$
\end{ndef}
The \emph{entropy} can trivially be expressed as:
$$
H(X) = - \sum_{x \in \X}\Prob_X (x)\log \Prob_X(x)
$$
There are some properties of the \emph{entropy} that must be remarked. 
\begin{nprop}
    Let $X$ be a random variable with image $\X$. then
    $$
0 \leq H(X) \leq \log(|\X|)
    $$
\end{nprop}
\begin{proof}
    Since $\log \ y$ is concave on $\R^+$, by Jensen's inequality , see ~\ref{prop:jensen},:
    $$
    H(X) = - \sum_{x \in X}\Prob_X (x)\log \Prob_X(x) \leq \log(\sum_{x \in \X} 1) = \log(|\X|)
    $$
    For the lower bound, it is easy to see that, since $\Prob_X(x) \in [0,1] \ \  \forall x \in \X $, and, hence , $\log \Prob_X(x) \leq 0 \ \ \forall x \in \X$. The product of both is negative, so we have a sum of negative terms that is changed its sign afterwards, so it is always $H(X) \geq 0$. 
\end{proof}
We can also see that the equality on the left holds if and only if $\exists x \in \X \ : P_X(x) = 1$. The right equality holds if and only if , forall $x \in \X$, $P_X(x) = \frac{1}{\abs{\X}}$

\subsection*{Conditional entropy}
We have already said that \emph{entropy measures} how surprising is that an event occurs. Usually, we will be looking at two random variables and it would be interesting to see how surprising is that one of them, say $X$, ocurred, if we already know that $Y$ ocurred. This leads us to the definition of \emph{conditional entropy}. Lets see a simpler case first:

Let $\mathcal A$ be an event, and $X$ a random variable. The conditional probability $\Prob_{X|A}$ defines the entropy of $X$ conditioned to $\mathcal A$:
$$
H(X|\mathcal A) = \sum_{x \in \X} \Prob_{X|A}(x) \log\frac{1}{\Prob_{X|A}(x)}
$$
If $Y$ is another random variable and $\mathcal Y$ is its image, intuitively we can sum the conditional entropy of an event with all the events in $\mathcal Y$, and this way we obtain the conditional entropy of $X$ given $Y$.
\begin{ndef}[Conditional Entropy]
Let $X,Y$ be random variables with images $\X,\mathcal Y$. The \emph{conditional entropy} $H(X | Y)$ is defined as:

\begin{equation*}
        \begin{split}
    H(X|Y) :=  & \sum_{y \in \mathcal Y} \Prob_{\mathcal Y}(y) H(X| Y = y)  \\ 
    & = \sum_{y \in \mathcal Y} \Prob_{\mathcal  Y}(y) \sum_{x \in \X} \Prob_{X | Y}(x|y)\log\frac{1}{\Prob_{X|Y}(x|y)}  \\
   & = \sum_{x \in X,y \in \mathcal Y}\Prob_{XY}(x,y)\log\frac{\Prob_Y(y)}{\Prob_{XY}(x,y)}
\end{split}
\end{equation*}



\end{ndef}
The interpretation of the \emph{Conditional Entropy} is simple: the uncertaincy in $X$ when $Y$ is given. Since we know about an event that has occurred ($Y$), intuitively the conditional entropy , or the uncertaincy of $X$ occurring given that $Y$ has occurred, will be lesser than the entropy of $X$, since we already have some information about what is happening. We can prove this:

\begin{nprop}
Let $X,Y$ be random variables with images $\mathcal X, \mathcal Y$. Then:
$$
0 \leq H(X|Y) \leq H(X)
$$
\end{nprop}

% Do I have to define it for the continuous case?
% Will I use the continuous or the discrete case?

