
We have seen how the framework presented in \cite{oord_et_al} uses a generative approach as a part of the representation learning process. Let us set in the case of learning representations of images. In this case, generative models must \emph{generate} each pixel on the image. This can be extremely computationally expensive. 

Until now, we had been trying to minimize the loss in \ref{NCE:loss}, which we proved that maximizes a lower bound in the Mutual Information. However, some papers such as \cite{chen_simple_2020}, \cite{tschannen_mutual_2020}, suggest that it is unclear if the success of their methods is caused by the maximization of mutual information between the latent representations, or by the specific form that the constrastive loss has.

In fact, in \cite{tschannen_mutual_2020} they provide empirical proof for the loose connection between the success of the methods that use MI maximization and the utility of the MI maximization in practice. They also empirically proof  that the encoder architecture can be more important than the estimator used to determine the MI.

Even with the empirically proved disconnection between MI maximization and representation quality, recent works that have used the loss function \ref{NCE:loss} have obtained state-of-art results in practice. There is an explanation for this, connecting the recently mentioned loss with a popular triplet loss.

\section{From deep metric learning to triplet losses}

We will consider sets of triplets $(x,y,z)$ where:
\begin{itemize}
\item The element $x$ is an archor point,
\item The element $y$ is a positive instance,
\item The element $z$ is a negative instance.
\end{itemize}
The main idea is to learn a representation of $x$, say $g(x)$, such that 
$$
\norm{g(x) - g(y)}_2 \leq \norm{g(x) - g(z)}_2,
$$
for each triplet in the set. It would be interesting to present the model non-trivial metric to the learning algorithm. When the representation $g$ improves, this is harder to do.

The InfoNCE loss \ref{NCE:loss} has proved to be useful in representation learning.Let us consider a reformulation on it. Firstly, since $f_k$ was an exponential, we can also consider $e^f$ and remove the exponential from $f$, this is just notation. Now, we can rewrite the InfoNCE objective as follows
\begin{align*}
I_{NCE} = E\left[ \frac{1}{N} \sum_{i = 1}^N \log \frac{e^{f(x_i,y_i)}}{\frac{1}{N}\sum_{j=1}^N e^{f(x_i,y_j)}}\right] = \log N - E\left[ \frac{1}{N} \sum_{i=1}^N \log \left( 1+ \sum_{j\neq i}e^{f(x_i,y_j)- f(x_i,y_i)}\right)\right].
\end{align*}

