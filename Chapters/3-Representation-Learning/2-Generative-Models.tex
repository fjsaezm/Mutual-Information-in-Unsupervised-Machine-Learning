
\label{Chapter:Gen:Models}
The vast majority of the problems in ML are usually of a discriminative nature, which is almost a synonym of supervised learning. However, there also exist problems that involve learning how to generate new examples of the data. More formally:

\begin{ndefC}
\begin{enumerate}
\item \emph{Discriminative models} estimate $p(y|x)$, the probability of a label $y$ given an observation $x$.
\item \emph{Generative models} estimate $p(x)$, the probability of observing the datapoint $x$. If the dataset is labeled, a generative model can also estimate the distribution $p(x|y)$.
\end{enumerate}

\end{ndefC}


From now on, let $\D$ be any kind of observed data. This will always be a finite subset of samples taken from a probability distribution $\pd$. There are models that, given $\D$, try to approximate the 
probability distribution that lies underneath it. These are called \emph{generative models (G.M.)}. 

Generative models can give parametric and non parametric approximations to the distribution $\pd$. 
In our case, we will focus on parametric approximations where the model searches for the parameters that minimize a chosen metric (which can be a distance or other kind of metric such as K-L divergence) between the model distribution and the data distribution. 

We can express our problem more formally as follows. Let $\theta$ be a generative model within a model family $\mathcal M$. The goal of generative models is to optimize:
$$
\min_{\theta \in \mathcal M} d(\pd,p_\theta),
$$
where $d$ stands for the distance between the distributions. We can use, for instance, K-L divergence.

Generative models have many useful applications. We can however remark the tasks that we would like our generative model to be able to do. Those are:
\begin{itemize}
\item Estimate the density function: given a datapoint, $x \in D$, estimate the probability of that point $p_\theta(x)$.
\item Generate new samples from the model distribution $x \sim p_\theta(x)$.
\item Learn useful features of the datapoints.
\end{itemize}

If we have a look again at the example of the zebras, if we make our generative model learn about images of zebras, we will expect our $p_\theta(x)$ to be high for zebra's images. We will also expect the model
to generate new images of this animal and to learn different features of the animal, such as their big size in comparison with cats.

\section{Autoregressive Models}

In time-series theory, autoregressive models use observations from previous time steps to predict values at the current time. 
Fixing an order of the variables $x_1,\dots,x_n$, the distribution for the $i$-th random variable depends on all the preceding values in the particular chosen order. We will make use of the name of these models to 
define the machine learning approach.

A very first definition of \emph{autoregressive models (AR)} would be the following one: \emph{autoregressive models are feed-forward models that predict future values using past values}. Let us go deeper into this 
concept and explain how it behaves.

Again, let $\D$ be a set of $n-$dimensional datapoints $x$. We can assume that $x \in \{0,1\}^n$ for simplicity, without losing generality. If we choose any $x\in \D$, using the chain rule of probability, we obtain
\[
p(x) = \prod_{i=1} ^n p(x_i | x_1,\dots,x_{i-1}) = \prod_{i = 1}^n p(x_i|\bm{x}_{<i}),
\]
where $\bm{x}_{<i} \in \R^{i-1}$ is a vector whose components are the previous $x_j$ for $j = 1,\dots, i-1$, that is: $\bm{x}_{<i}= [x_1,\dots, x_{i-1}]$. \\
It is known that given a set of discrete and mutually dependent random variables, they can be displayed in a table of conditional probabilities. If $K_i$ is the number of states that each random variable can take
then $\prod K_i$ is the number of cells that the table will have. If we represent $p(x_i|\bm{x}_{<i})$ for every $i$ in tabular form, we can represent
any possible distribution over $n$ random variables. 

This, however, will cause an exponential growth on the complexity of the representation, due to the need of specifying $2^{n-1}$ possibilities 
for each case. In terms of neural networks, since each column must sum $1$ because we are working with probabilities, we have $2^{n-1}-1$ parameters for this conditional, and the tabular representation
becomes impractical for our network to learn when $n$ increases.

In autoregressive generative models, the conditionals are specified as we have mentioned before: parameterized functions with a fixed numbers of parameters. More precisely,  we assume 
the conditional distributions to be Bernoulli random variables and learn a function $f_i$ that maps these random variables to the mean of the distribution. Mathematically, we have to find 
$$
p_{\theta_i}(x_i | \bm{x}_{<i}) = \operatorname{Bern}(f_i(x_1,\dots,x_{i-1})),
$$
where $\theta_i$ is the set of parameters that specify the mean function $f_i:\{0,1\}^{i-1} \to [0,1]$.

Then, the number of parameters is reduced to $\sum_{i=1}^n \abs{\theta_i}$ so we can not represent all possible distributions as we could when using the tabular form of the conditional probabilities.
We are now setting the limit of its expressiveness because we are setting the conditional distributions $p_{\theta_i}(x_i|\bm{x}_{<i})$ to be \emph{Bernoulli} random variables with the mean specified by a restricted class 
of parametrized functions. 

Let us see a very simple case first in order to understand it better. Let $\sigma$ be a \emph{sigmoid}\footnotemark non linear function and 
$\theta_i = \left\{\alpha_{0}^{(i)},\alpha_{1}^{(i)},\dots, \alpha_{i-1}^{(i)}\right\}$ the parameters of the mean function. Then, we can define our function $f_i$ as :
$$
f_i(x_1,\dots, x_{i-1}) = \sigma(\alpha_{0}^{(i)} + \alpha_{1}^{(i)}x_1 + \dots + \alpha_{i-1}^{(i)}x_{i-1}).
$$
%------------- Footnotemark
\footnotetext{A sigmoid function is a bounded,differentiable, real function which derivative is non-negative at each point and it has exactly one inflection point.}
%----------------------
In this case, the number of parameters would be $\sum_{i = 1}^n i = \frac{n(n+1)}{2}$, so using \emph{Big }$O$ notation, we would be in the case of $O(n^2)$. We will state now a more general and useful case,
giving a more interesting parametrization for the mean function: \emph{multi layer perceptrons}\footnotemark (MLP).

%------------- Footnotemark
\footnotetext{Multi layer perceptrons are feed-forward neural networks with at least 3 layers: input, hidden and output layers; each one using an activation
function.}
%----------------------

For this example we will consider the most simple MLP: the one with one hidden layer. Let $h_i = \sigma(\bm{A}_i \bm{x}_{<i} + c_i)$ be the hidden layer activation function. Remember that $h_i \in \R^d$. Let
$ \theta_i = \{ \bm{A}_i \in \R^{d \times (i-1)}, \ c_i \in \R^d, \ \alpha^{(i)} \in \R^d, \ b_i \in \R\}$ the set of parameters
for the mean function $f_i$, that we define as:
$$
f_i(\bm{x}_{<i}) = \sigma(\alpha^{(i)}h_i + b_i).
$$
In this case, the number of parameters will be $O(n^2 d)$.

% Link NADE http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf

% LINK RNADE https://arxiv.org/pdf/1306.0186.pdf
This is the simplest example. Currently, there are alternative parametrization models , such as the \emph{Neural Autoregressive Density Estimator} \citep{larochelle_neural_nodate}, that provide a more statistically and computationally efficient solution. In fact, the number of parameters is reduced from $O(n^2 d)$ to $O(nd)$. Also, \emph{RNADE} \citep{uria_rnade_2014} extends NADE to learn generative models over real-valued data, generalizing the case that we have just exposed. However, these models are out of the scope of this project so no further explanation will be given.

