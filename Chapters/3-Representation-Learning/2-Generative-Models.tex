
\label{Chapter:Gen:Models}
The vast majority of the problems in ML are usually of a discriminative nature, which is almost a synonym of supervised learning. However, there also exist problems that involve learning how to generate new examples of the data. More formally:

\begin{ndefC}
\begin{enumerate}
\item \emph{Discriminative models} estimate $p(y|x)$, the probability of a label $y$ given an observation $x$.
\item \emph{Generative models} estimate $p(x)$, the probability of observing the datapoint $x$. If the dataset is labeled, a generative model can also estimate the distribution $p(x|y)$.
\end{enumerate}

\end{ndefC}


From now on, let $\D$ be any kind of observed data. This will always be a finite subset of samples taken from a probability distribution $\pd$. There are models that, given $\D$, try to approximate the 
probability distribution that lies underneath it. These are called \emph{generative models (G.M.)}. 

Generative models can give parametric and non parametric approximations to the distribution $\pd$. 
In our case, we will focus on parametric approximations where the model searches for the parameters that minimize a chosen metric (which can be a distance or other kind of metric such as K-L divergence) between the model distribution and the data distribution. 

We can express our problem more formally as follows. Let $\theta$ be a generative model within a model family $\mathcal M$. The goal of generative models is to optimize:
$$
\min_{\theta \in \mathcal M} d(\pd,p_\theta),
$$
where $d$ stands for the distance between the distributions. We can use, for instance, K-L divergence.

Generative models have many useful applications. We can however remark the tasks that we would like our generative model to be able to do. Those are:
\begin{itemize}
\item Estimate the density function: given a datapoint, $x \in D$, estimate the probability of that point $p_\theta(x)$.
\item Generate new samples from the model distribution $x \sim p_\theta(x)$.
\item Learn useful features of the datapoints.
\end{itemize}

If we have a look again at the example of the zebras, if we make our generative model learn about images of zebras, we will expect our $p_\theta(x)$ to be high for zebra's images. We will also expect the model
to generate new images of this animal and to learn different features of the animal, such as their big size in comparison with cats.

We have to remark an example of generative models since it will be mentioned later. In time-series theory, \emph{autoregressive models (AR)} are feed-forward models that predict future values using past values. 
