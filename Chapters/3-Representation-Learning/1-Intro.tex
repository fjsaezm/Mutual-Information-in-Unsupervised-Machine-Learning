


Before continuing to present the mathematical notions that are neccesary for the topics that are treated in this work, it is interesting to present what we are trying to achieve.\\

\emph{Machine learning} is the field of computer science that studies algorithms that improve automatically through experience from examples. 
These algorithms allow computers to discover how to
perform tasks without being explicitly programmed to do them. For the computers to learn, it is mandatory that a finite set of data (or dataset) $\D$ is available. \\

Whenever a computer is providen with data, the data can be \emph{labeled} or \emph{unlabeled}. Labeled data is the one that, each point $x_i \in \D$ is related to a tag $y_i \in Y$, where $Y$ is a set of classes.
Unlabeled data is the kind of data that does not have a label or class associated to it, so it is just $x_i \in \R^d$. 

Depending on how the data (\emph{or signal}) is given to the computer, the machine learning approaches can be divided into three broad categories:
\begin{enumerate}
    \item \emph{Supervised learning}. In this category the goal is to use the labeled data in order to find a function that maps the dataset to the set of classes. That is a function $g:\D \to Y$. 
    An example of supervised learning is image classification: giving a label to an image.
    \item \emph{Unsupervised learning}. In ths case, the data is unlabeled, so the approach is completely different. Usually, the goal here is to discover hidden patterns in data or to learn features from it.
    An example of this kind of learning is K means, which consistis in clustering the data in $k$ groups.
    \item \emph{Reinforcement learning}. This is the area concerned with how intelligent agents take decisions in a specific environment in order to obtain the best reward in their objective. This kind of learning is used

\end{enumerate}

In this work, we will focus on unsupervised learning. Particularly, in representation learning.\\


There are many different tasks that can be performed with the data, such as linear regression, logistic regression, or classification. In any of these tasks, computers might need
to do intermediate steps before giving a label to the input example. Sometimes, they must create a \emph{representation} that 
contains the data's key qualities.  Here is where \emph{representation learning} is born. 


Intuitively, if $x$ is a datapoint in a dataset $\D \subset \R^d$, a \emph{representation} of $x$ is a vector $r \in \R^n$ (usually, $n \leq d$), that shares information with the datapoint $x$. That is, if a machine learning model
tries to make a posterior task, such as classification, the input $x$ must be transformed to a, usually lower dimensional, vector $r$ in order to perform the final label.

\emph{Features} are parts or patterns of an datapoint $x \in \D$ that help to identify it. In fact, this attribute is shared by all independent units that represent the same object. For instance, if we consider an image of any square, we should be able to identify 4 corners and 4 edges. These could be features of a square.
When we mention feature detection, we are adressing the methods for detecting these features of a datapoint.

Representation learning is a set of techniques that allow a system to discover the representations needed for feature detection or classification. 
In contrast to manual feature engineering (which involves manually exploring the data and finding relationships in it), feature learning allows a machine to learn the features and to use them to perform a task.\\

Feature learning can be supervised or unsupervised. In supervised feature learning, representations are learned using labeled data.
Examples of this kind of feature learning are supervised neural networks and multilayer perceptron. In unsupervised learning, the features are learned using unlabeled data. 
There are many examples of this, such as independent component analysis (ICP) and autoencoders. In this work, we will be working with unsupervised feature learning.\\

The performance of machine learning methods is heavily dependent on the choice of data features \citep{bengio_representation_2014}. This is why most of the current 
effort in machine learning focuses on designing preprocessing and data transformation that lead to good quality representations. A representation will be of good quality when its features
produce good results when we evaluate the \emph{accuracy} of our model.\\

The main goal in representation learning is to obtain features of the data that are generally good for either of the supervised tasks. That is, we would like to obtain
a representation that is either good for image classification (giving an image a label of what we can see in it) or image captioning (producing a text that describes the image).\\

Data's features that are invariant through time are very useful for machine learning models. In \citep{wiskott_slow_2002}, \emph{slow features} are presented. Slow features are defined as features of a signal 
(which can be the input of a model) that vary slowly during time. That means, if $\bm{X}$ is a \emph{time series}\footnotemark, we will try to find any number of features in $\bm{X}$ that vary the most slowly.
These kind of features are the most interesting ones when creating representations, since they give an abstract view of the original data.\\


%------------- Footnotemark
\footnotetext{A time series is an ordered sequence of values of a random variable at, usually, equally spaced time intervals.}
%----------------------

Let us give an example: In computer vision, the value of the pixels in an image can vary fast. For instance, if we have a zebra on a video and the zebra is moving from one side of the image to the other, due 
to the black stripes of this animal, the pixels will fast change from black to white and viceversa, so value of pixels is probably not a good feature to choose as an slow feature. However, there will always
be a zebra on the image, so the feature that indicates that there is a zebra on the image will stay positive throughout all the video, so we can say that this is a slow feature.\\

We will be studying different models that try to learn representations from raw data without labels, as we have mentioned. We usually need a function that measures what is the penalty that the model gets for a choice of a parameter.
This is called a \emph{loss function}, that we will want to optimize.

For instance, in a regression problem, a good example of loss function is \emph{mean squared error}, which is expressed as follows:
\[
\text{MSE} = \frac{\sum_{i =1 }^n \left(y_i - \hat{y_i}\right)^2}{n}.
\]
In a classification problem, each datapoint $x_i$ has a correct classification $y_i$. In this case, the score of the correct category $y_i$ should be greater than the sum of the scores of all incorrect categories $y_j$ with $j \neq i$,
 so we could use a function like \emph{support vector machine (SVM) loss}:
\[
\text{SVMLoss} = \sum_{j \neq y_i} \max(0,s_j - s_{y_i} + 1)
\]


In the context of machine learning, a \emph{model} is the result of running a maching learning algorithm in data. This model will represent what the computer has learned from the data using this algorithm. As an easy example, when we use the
linear regression algorithm, we obtain a model that is a vector of coefficients with specific values.

In the following chapters, we will explain different kinds of neural networks and which kind of loss functions they use. 


