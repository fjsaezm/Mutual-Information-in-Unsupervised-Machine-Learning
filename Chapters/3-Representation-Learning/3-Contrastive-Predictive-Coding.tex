We are now ready to connnect the concepts of mutual information and generative models that we have presented. In unsupervised learning,
it is a common strategy to predict future information and to try to find out if our predictions are correct.
In \emph{natural language processing},for instance, representations are learned 
using neighbouring words \citep{mikolov_efficient_2013}, and in images, some studies have been able to predict color from grey-scale \citep{doersch_unsupervised_2016}.

When we talk about high-dimensional data, it is not useful to make use of an unimodal loss function to evaluate our model. If we did it like this, we would be assuming that there is only
one peak in the distribution function and that it is actually similar to a Gaussian.  This is not always true, so we can not assume it for our models. Generative models can be used for this purpose:
they will model the relationships in the data $x$. However,they ignore the context $c$ in which the data $x$ is involved. As an easy example of this, an image contains thousands of bits of information,
while the label that classifies the image contains much less information , say, $10$ bits for $1024$ categories. Because of this, modeling $p(x|c)$ might not be the best way to proceed if we want
to obtain the real distribution that generates our data. 

Our goal here will be to seek for a way of extracting shared information between the context and the data. Due to the differences in data dimensionality, if we want to predict the future $x$ using the context $c$, firstly we must
encode our entry data $x$ into a representation which size is comparable to context size. Firstly, an \emph{encoder} is used. An encoder is a model that, given an input $x$, provides a feature map or vector that holds the information
that the input $x$ had. In fact, and here is where we link the mutual information with the current topic, we want our encoder to maximize
$$
I(x,c) = \sum{x,c}p(x,c)\log\frac{p(x|c)}{p(x)}
$$
that is, the mutual information between the input $x$ and the context $c$.  Maximizing the mutual information between $x$ and $c$, we extract the latent variables
that the inputs have in common.

So, we will use an encoder $g_{enc}$ that transforms the input sequence of observations $x_t$ to a sequence of latent representations
$$
z_t = g_{enc}(x_t).
$$
After we have obtained $z_t$, we use it as input of an autoregressive model to produce a context latent representation:
$$
c_t = g_{ar}(z_{\leq t}).
$$
In this case, $c_t$ will summarize the information of $z_i$ for $i \leq t$. Following the argument that we gave before, predicting the future $x_{t+k}$ using only a generative model, say $p_k(x_{t+k}|c)$
might not be correct, since we would be ignoring the context. Let us see how we train the encoder $g_{enc}$ and the autoregressive model $g_{ar}$.

Let $X = \{x_1,\dots,x_N\}$ be a set of $N$ random samples. $X$ will contain positive sample taken from the distribution $p(x_{x+k}|c_t)$ and $N-1$ negative
samples from the distribution propused $p(x_{t+k})$. With this set, we would like to optimize the following loss function:
$$
\mathcal L_N = - \underset{X}{E} \left[\log \frac{f_k(x_{t+k},c_t)}{\sum_{x_j \in X} f_k(x_k,c_k)}\right].
$$


This is no more than the known \emph{categorical cross-entropy} of classifying the positive sample correctly, with the argument of the logarithm being the prediction
of the model. The optimal probability for this loss can be written as $p(d = i|X,c_t)$, letting $[d = i]$ the indicate that the sample $x_i$ is the positive sample. Now, the probability that $x_i$ was drawn from the conditional
distribution $p(X_{t+k}|c_t)$ rather than the proposal distribution $p(x_{t+k})$, leads us to the following expression:
$$
p(d = i | X , c_t) = \frac{p(x_i|c_t) \prod_{l \neq i}p(x_l)}{\sum_{j=1}^N p(x_j|c_t) \prod_{l \neq j} p(x_l)} = 
$$