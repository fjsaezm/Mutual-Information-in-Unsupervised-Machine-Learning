Statistical inference is the process of deducing properties of an underlying distribution by analyzing the data that it is available. With this purpose, techniques like deriving estimates and testing hypotheses are used. 

Inferential statistics are usually contrasted with descriptive statistics, which are only concerned with properties of the observed data. The difference between these two is that in inferential statistics, we assume that the data comes from a larger
population that we would like to know.

In \emph{machine learning}, subject that concerns us the most, the termi inference is sometimes used to mean \emph{make a prediction by evaluating an already trained model}, and in this context, inferring properties of the model is refered as \emph{training or learning}.

\section{Parametric Modeling}

In the following chapters, we will be trying to estimate density functions in a dataset. To do this we will be using \emph{parametric models}. We say that a \emph{parametric model}, $p_\theta(x)$, 
is a family of density functions that can be described using a finite numbers of parameters $\theta$. We can get to the concept of \emph{log-likelihood} now.

\begin{ndef}
The \emph{likelihood} $\mathcal L(\theta | x)$ of a parameter set $\theta$ is a function that measures how plausible is $\theta$, given an observed point $x$ in the dataset $\D$. It is defined as the value of the 
density function parametrized by $\theta$ at $x$. That is:
$$
\mathcal L(\theta|x) = p_\theta(x).
$$
\end{ndef}

In a finite dataset $\D$ consisting of independent observations, we can write:
\[
\mathcal L(\theta | X) = \prod_{x \in D} p_\theta(x).
\]

This can be computationally hard to work with, so the log-likelihood is often used instead.

\begin{ndef}
Let $\D$ be a dataset of independent observations and $\theta$ a set of parameters. Then, we define the \emph{log-likelihood} $\ell$ as the sum of the logarithms of the evaluations of $p_\theta$ in each $x$ in the dataset. That is:
\[
\ell (\theta | X) = \sum_{x \in \D} \log p_\theta(x).
\]
\end{ndef}

Our goal would be to find the optimal value $\hat{\theta}$ that maximizes the likelihood of observing the dataset $\D$. We get to the following definition:

\begin{ndef}
    We say that $\hat{\theta} = \hat\theta (\D)$ is a \emph{maximum likelihood estimator}(MLE) for $\theta$ if  
    $$
    \hat\theta \in \argmax_{\theta} \mathcal L(\theta | \D)
    $$
    for every observation $\D$. 
\end{ndef}

\section{Minimal sufficient statistics}

In parametric modeling, the goal was to determine the density function under a distribution. Another interesting task can be determining specific parameters or quantitys related to a distribution, given a sample $X = (x_1,\cdots,x_n)$.

\begin{ndef}
    A \emph{statistic} is a measurable function of the data. That is, if $T : \Omega \to \mathbb T$ is measurable, then $T(X)$ is a statistic.
\end{ndef}

However, not all statistics will provide useful information for the statistical inference problem. We would like to find statistics that provide relevant information.

\begin{ndef}
    Let $X \sim P_\theta$. Then, the statistic $T(X) = T : (\Omega, \Alg) \to (\mathbb T, \mathcal B)$, is sufficient for a family of parameters $\{P_\theta \ : \ \theta \in \Theta \}$ if the conditional distribution of $X$, given $T = t$, is indepentent of $\theta$.\\

    Alternatively, we can say $T(X)$ is sufficient for $\theta$ if its mutual information with $\theta$ equals the mutual information between $X$ and $\theta$, that is:
    $$
    I(\theta, X) = I (\theta, T(X))
    $$
\end{ndef}

The easiest example of a sufficient statistic is the mean $\mu$ of a gaussian distribution with known variance. Oppositely, the \emph{median} of an arbitrary distribution
is not sufficient for the mean since, even if the median of the sample is known, more information about the mean of the population can be obtained from the mean of the sample itself.

Although it will not be shown in this document, sufficient statistics are not unique. In fact, if $T$ is sufficient, $\psi(T)$ is sufficient for any bijective mapping $\psi$. It would be interesting to find a sufficient statistic $T$ that is \emph{the smallest} of them.

\begin{ndef}
    A sufficient statistic $T$ is minimal if, for every sufficient statistic $U$, there exists a mapping $f$ such that $T(x) = f(U(x))$ for any $x \in \Omega$.
\end{ndef}