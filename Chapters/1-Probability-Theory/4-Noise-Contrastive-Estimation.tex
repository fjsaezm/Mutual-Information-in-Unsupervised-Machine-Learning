\label{Chapter:NCE}
Our problem now is, to estimate the density function (p.d.f.) of some observed data.

A sample $X = \{x_1,\dots,x_{T_d}\}$ of a random vector is observed. It follows an unknown pdf $P_d$. We assume that the data p.d.f. belongs to a parametrized family of functions, that is
\[
P_d \in \{P_m(.;\theta)\}_\theta,
\]
where $\theta$ is a vector of parameters. This means that, in fact,
$$
P_d(.) = P_m(.;\theta^*) \quad \text{for some } \theta^*,
$$
so our problem is to find $\theta^*$. 

Any estimate $\hat{\theta}$ must meet the constraints that a normalized p.d.f. should satisfy, that is:
$$
\int P_m(u;\hat{\theta})du = 1, \quad \quad P_m(.;\hat{\theta})\geq 0.
$$
If the constraints are satisfied for any $\theta$ in the set of parameters, we say that the model is normalized, and then we can use the maximum likelihood principle to estimate $\theta$.

Let us assume that the noisy data $Y$ is an i.i.d. sample $\{y_1,\dots,Y_{T_n}\}$ of a random variable with p.d.f. $P_n$. The ratio $P_d/P_n$ of the density functions that generate $X$ and $Y$ respectively, can give us a relative description of the data $X$. If $P_n$ is known, then we can obtain $P_d$ using the ratio that we have just mentioned.

In order to discriminate between elements of $X$ and $Y$, it is needed to compare their properties. We will show that we can provide a relative description of $X$ in the form of an estimate of the ratio $P_d/P_n$.

Let $U = \{u_1,\cdots,u_{T_d + T_n}\}$ be the union of the sets $X$ and $Y$. We assign to each $u_t$ a binary class label:
\[
C_t(u_t) = \begin{cases}
1 & if \ u_t \in X\\
0 & if \ u_t \in Y
\end{cases}
\]
We will now make use of logistic regression, where the posterior probabilities of the classes given the data are estimated. We know that $P_d$ is unknown, we want to model $P(.|C=1)$ with $P_m(.;\theta)$. Note that $\theta$ may include a parameter for the normalization of the model, if it is not normalized. Hence, we have:
\[
P(u|C = 1,\theta) = P_m(u;\theta), \quad \quad P(u|C = 0) = P_n(u),
\]
with
\[
P(C = 1) = \frac{T_d}{T_d + T_n}, \quad \quad P(C = 0) = \frac{T_n}{T_d + T_n}.
\]
Hence, if $\nu = T_n/T_d$, the posterior probabilities for the classes are:
\[
P(C=1|u;\theta) = \frac{P_m(u;\theta)}{P_m(u;\theta) + \nu P_n(u)}, \quad \quad P(C = 0|u; \theta) = \frac{\nu P_n(u)}{P_m(u;\theta) + \nu P_n(u)}.
\]
Denote $G(.;\theta)$ to the log ratio between $P_m(.;\theta)$ and $P_n$:
\begin{equation}\label{log:ratio:G}
G(u;\theta) = \log P_m(u;\theta) - \log P_n(u) = \log \frac{P_m(u;\theta)}{P_n(u)}.
\end{equation}
Also, let $r_\nu$ the logistic function parametrized by $\nu$, that is:
\begin{equation}\label{log:func:nu}
r_\nu(u) = \frac{1}{1 + \nu exp(-u)}.
\end{equation}
Using \ref{log:ratio:G} and \ref{log:func:nu}, we can write
\[
h(u;\theta) := P(C = 1|u ; \theta) =    r_\nu(G(u;\theta)) = \frac{1}{1+ \nu exp(\log \frac{P_m(u,\theta)}{P_n(u)})}. 
\]
Since the class labels $C_t$ are assumed Bernoulli distributed and independent, the conditional log-likelyhood has the form:
\begin{equation}\label{log:likelihood:theta}
\ell(\theta)  = \sum_{t = 1}^{T_d + T_n} C_t \log P(C_t = 1|u_t; \theta) + (1-C_t) \log P(C_t = 0|u_t;\theta).
\end{equation}
Now, in the $t$ such that $u_t$ in $X$,then  $u_t = x_t$ an we have that $P(C_t = 0|x_t;\theta) = 0$, so we obtain that the term that adds to the sum in that certain $t$ is:
\[
1\cdot \log P(C_t = 1|u_t;\theta) = \log h(x_t;\theta).
\]
Using the same argument for $t$ such that $u_t \in Y$, we obtain the following form of the log-likelyhood in \ref{log:likelyhood:theta}:
\begin{equation}\label{log:likelihood:red}
\ell(\theta) = \sum_{t = 1}^{T_d} \log [h(x_t;\theta)] + \sum_{t = 1}^{T_n} \log[1- h(y_t,\theta)].
\end{equation}
Now, optimizing $\ell(\theta)$ with respecto to $\theta$ leads to an estimate $G(.;\hat{\theta})$ of the log-ratio $\log (P_d/P_n)$, so we get an approximate description of $X$ relative to $Y$ by optimizing \ref{log:likelyhood:red}.

If we consider $-\ell(\theta)$, this is known as the \emph{cross entropy function}.

\begin{remark}
    Here, we have achieved the estimation of a p.d.f. , which is an unsupervised (not labeled data) learning problem, logistic regression, which is supervised learning (labeled data).
\end{remark}

Now, if we consider $P_m^0(.;\alpha)$ an unnormalized (doest not integrate $1$) model, we can add a normalization parameter to it in order to normalize it. We can consider
\[
\log P_m(.;\theta) = \log P_m^0(.;\alpha ) + c  , \quad \quad \text{with } \theta=(\alpha,c).
\]
With this model, a new estimator is defined. Considering $X$ as before and $Y$ an artificially generated set with $T_n = \nu T_d$ independent obvservations extracted from $P_n$, known. as the argument $\hat{\theta}_T$ which maximizes
\[
J_T(\theta) = \frac{1}{T_d}\{\sum_{t = 1}^{T_d} \log[h(x_t;\theta)] + \sum_{t=1}^{T_n}\log[1-h(y_t;\theta)]\}.
\]

We have to remark that in this case, we have fixed $\nu$ before $T_n$, so $T_n$ will increase as $T_d$ increases. Now, using the weak law of large numbers, $J_T(\theta) \to J$ in probability, where
\[
J(\theta) = E\{\log[h(x;\theta)]\} + \nu E{\log[1-h(y;\theta)]}.
\]
Let us rename some terms before announcing a theorem. We want to see $J$ as a function of $\log P_m(.;\theta)$ instead of only $\theta$. In order to do this, let $f_m(.) = \log P_m(.;\theta)$, and consider
\[
\tilde{J}(f_m) = E\{\log[r_\nu (f_m(x) - \log P_n(x))]\} + \nu E\{\log [1- r_\nu(f_m(y) - \log P_n(y))]\}.    
\]
The following theorem states that the probability density function $P_d$ of the data can be found by maximizing $\tilde{J}$, that is, learning a nonparametric classifier in \emph{infinite data}.

\begin{nth}
The objective $\tilde{J}(f_m)$ achieves a maximum at $f_m = \log P_d$. Furthermore, there are not other extrema if the noise density $P_n$ is chosen such that it is nonzero whenever $P_d$ is nonzero.
\end{nth}