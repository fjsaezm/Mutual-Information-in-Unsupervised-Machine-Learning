We have introduced the concepts of \emph{random variable},  \emph{random vector} and its \emph{probability distribution}.
Now, given two distributions, in the following chapters we will like to see how different they are from each other.
In order to compare them, we enunciate the definition of the Kullback-Leibler divergence.

\begin{ndef}[Kullback-Leibler Divergence]
Let $P$ and $Q$ be probability distributions over the same probability space $\Omega$. Then, the Kullback-Leibler divergence is defined as:
$$
D_{KL}(P \ || \ Q) = E_P\left[\log{\frac{P(x)}{Q(x)}}\right].
$$
\end{ndef}
It is defined if, and only if $P$ is \emph{absolutely continuous with respect to} $Q$, that is , if $P(A) = 0$ for any $A$ subset of $\Omega$ where $Q(A) = 0$.
 There are some properties of this definition that must be stated. The first one is the following proposition:

\begin{nprop}
If $P,$ $Q$ are two probability distributions over the same probability space, then $D_{KL}(P|Q) \geq 0$.
\end{nprop}
\begin{proof}
Firstly, note that if $a \in \R^+$, then $\log \ a \leq a-1$. Then:
\begin{align*}
-D_{KL}(P \ || \ Q) & = - E_P\left[\log{\frac{P(x)}{Q(x)}}\right] \\
             & = E_P\left[\log{\frac{Q(x)}{P(x)}}\right] \\
             & \leq E_P\left[\left(\frac{Q(x)}{P(x)} - 1\right)\right]\\
             & = \int P(x) \frac{Q(x)}{P(x)} dx -1 \\
             & = 0.
\end{align*}
So we have obtained that $-D_{KL}(P\ ||\ Q) \leq 0$, which implies that $D_{KL}(P\ || \ Q) \geq 0$.
\end{proof}
As a corollary of this proposition, we can affirm that $D_{KL}(P\ ||\ Q)$ equals zero if and only if $P = Q$ almost everywhere. 
We will also remark the discrete case, as it will be used later. Let $P,Q$ be discrete probability distributions defined on the same probability space $\Omega$. Then, 
$$
D_{KL}(P\ ||\ Q) = \sum_{x \in \Omega} P(x) \log \left( \frac{P(x)}{Q(x)}\right)
$$

\section{Examples of distributions}

Some examples of common distributions will now be presented. They will be used further in this document.

\subsection{Bernoulli}

Imagine that you want to model the possible outcomes of an experiment with two possibilites: sucess or failure. Imagine also that you already know that in your experiment there is a probability $p$ of 
achieving success. That is the intuitive idea of a Bernoulli distribution. We can define it more formally as it follows: 

The Bernoulli distribution is a discrete probability distribution of a random variable that takes two values, $\{0,1\}$, with probabilities $p$ and $q = 1-p$, respectively. We will say that our distribution is a $Bern(p)$.

If $k$ is a possible outcome, we can define
the probability mass function $f$ of a Bernoulli distribution as:
$$
f(k,p) = 
\begin{cases} 
p \quad & \text{ if } k=1,\\
1-p \quad & \text{ if} k = 0
\end{cases}
$$
Using the expression of the mean for discrete random variables, we obtain that $E[X] = p$ and 
$$
\Var[X] = E[X^2] - E[X]^2 = E[X] - E[X]^2 = p-p^2 = p(1-p) = pq.
$$

As a note, this is just a particular case of the \emph{Binomial distribution} with $n=1$.