

Underneath each experiment involving any grade of uncertainty there is a \emph{random variable}. This is no more than a \emph{measurable} function between two \emph{measurable spaces}.
A probability space is composed by three elements: $(\Omega, \Alg, \Prob)$. We will define those concepts one by one.

\section{Basic notions}

\begin{ndef}Let $\Omega$ be a non empty sample space. $\Alg$ is a $\sigma-$algebra over $\Omega$ if it is a family of subsets of $\Omega$ that verify that the emptyset is in $\Alg$, and it is closed under complementation and countable unions. That is:
\begin{itemize}
  \item $\emptyset \in \Alg$
  \item If $A \in \Alg$, then $\Omega \textbackslash A \in \Alg$
  \item If $\{A_i\}_{i \in \mathbb N} \in A$ is a numerable family of $\Alg$ subsets, then $\cup_{i \in \mathbb N} A_i \in \Alg$
\end{itemize}
\end{ndef}


The pair $(\Omega,\Alg)$ is called a \emph{measurable space} To get to our probability space, we need to define a \emph{measure} on the \emph{measurable space}.

\begin{ndef}
Given $(\Omega,\Alg)$, a measurable space, a \emph{measure} $\Prob$ is a countable additive, non-negative set function on this space. That is: $\Prob: \Alg \to \mathbb R_0^+$ satisfying:
\begin{itemize}
  \item $\Prob(A) \geq \Prob(\emptyset) = 0$ for all $A \in \Alg$
  \item $P(\cup_n A_n) = \sum_n P(A_n)$ for any countable collection of disjoint sets $A_n \in \Alg$.
\end{itemize}
\end{ndef}

If $\Prob(\Omega) = 1$, $\Prob$ is a \emph{probability measure} or simply a \emph{probability}. With the concepts that have just been explained, we get to the following definition:

\begin{ndef}
A \emph{measure space} is the tuple $(\Omega, \Alg,\Prob)$ where $\Prob$ is a \emph{measure} on $(\Omega, \Alg)$. If $\Prob$ is a \emph{probability measure} $(\Omega,\Alg,\Prob)$ will be called a \emph{probability space}.
\end{ndef}

Throughout this work, we will be always in the case where $\Prob$ is a probability measure, so we will always be talking about probability spaces. Some notation for these measures must be introduced. Let $A$ and $B$ be two events.
The notation $P(A,B)$ reffers to the probability of the intersection of the events $A$ and $B$, that is: $P(A,B) := P(A\cap B)$.
 It is clear that since $A \cap B = B \cap A$, then $P(A,B) = P(B,A)$. We remark the next definition since it will be important.

\begin{ndef}
Let $A,B$ be two events in $\Omega$. The \emph{conditional probability} of $B$ given $A$ is defined as:
$$
P(B|A) = \frac{P(A,B)}{P(A)}
$$
\end{ndef}



% Introduce here Bayes Theorem
% ------------------------------------------------------------------------------

There is an alternative way to state the definition that we have just made.

\begin{nth}[Bayes' theorem]
Let $A,B$ be two events in $\Omega$, given that $P(B) \neq 0$. Then
$$
P(B|A) = \frac{P(A|B) P(A)}{P(B)}
$$
\end{nth}
\begin{proof}
Straight from the definition of the conditional probability we obtain that:
$$
P(A,B) = P(A|B)P(B)
$$
We also see from the definition that
$$
P(B,A) = P(B|A)P(A)
$$
Hence, since $P(A,B) = P(B,A)$,
$$
P(A|B)P(B) = P(B|A)P(A) \implies P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$
\end{proof}


However, events might not give any information about another event occurring. When this happens, we call those events to be \emph{independent}. Mathematically, if $A$,$B$ are independent events:
$$
P(A,B) = P(A)P(B)
$$
and as a consequence of this, the conditional probabilty of those events is $P(A|B) = P(A)$.\\


\emph{Random variables} (R.V.) can now be introduced. Their first property is that they are measurable functions. Those kind of functions are defined as it follows:

\begin{ndef}
Let $(\Omega_1, \Alg),(\Omega_2, \mathcal B)$ be measurable spaces. A function $f: \Omega_1 \to \Omega_2$ is said to be \emph{measurable} if, $f^{-1}(B) \in \Alg$ for every $B \in \mathcal B$.
\end{ndef}

As a quick note, we can affirm that if $f,g$ are real-valued measurable functions, and $k \in \mathbb R$, it is true that $kf$, $f+g$ , $fg$ and $f/g$ (if $g$ is not the identically zero function) are also \emph{measurable functions}.

We are now ready to define one of the concepts that will lead us to the main objective of this thesis.

\begin{ndef}[Random variable]
Let $(\Omega,\Alg,\Prob)$ be a probability space, and $(E,\mathcal B)$ be a measurable space. 
A \emph{random variable} is a measurable function $X: \Omega \to E$, from the probability space to the measurable space. This means: for every subset $B \in (E,\mathcal B)$, its preimage
$$
X^{-1}(B) = \{\omega : X(\omega) \in B\} \in \Alg .
$$
\end{ndef}

Using that sums, products and quotients of measurable functions are measurable functions, we obtain that \emph{sums, products and quotients of random variables are random variables}.

Let now $X$ be a R.V. The \emph{probability} of $X$ taking a concrete value on a measurable set contained in $E$, say, $S \in E$, is written as:
$$
P_X(S) = P(X \in S) = P(\{a \in \Omega : X(a) \in S\})
$$

A very simple example of random variable is the following:

\begin{nexample}
  Consider tossing a coin. The possible outcomes of this experiment are \emph{Heads or Tails}. Those are our random events. We can give our random events a possible value. For instance, let \emph{Heads} be $1$ and \emph{Tails} be 0. Then, our random variable looks like this:
  \begin{equation*}
      X  = \left\{ \begin{aligned}
  1 & \text{if we obtain heads} \\
  0 & \text{if we obtain tails}
\end{aligned}\right.
  \end{equation*}

\end{nexample}

In the last example, our random variable is \emph{discrete}, since the set $\{X(\omega): \omega \in \Omega\}$ is finite.
 A \emph{Random Variable} can also be \emph{continuous}, if it can take any value within an interval.\\


\section{Expectation of a random variable}

\begin{ndef}
The \emph{cumulative distribution function } $F_X$ of a real-valued random variable $X$ is its probability of taking value below or equal to $x$. That is:
$$
F_X(x) = P(X \leq x) = P(\{\omega : X(\omega) \leq x\}) = P_X((-\infty,x]) \quad \forall x \in \mathbb R
$$
\end{ndef}

Depending on the image of a random variable $X$, we can difference between certain types of random variables. If the image $\mathcal X$ of $X$ is countable, we call it a \emph{discrete} random variable. Its \emph{probability mass function} gives the
probability of the r.v. being equal to a certain value:
$$
p(x) = P(X = x).
$$
If the image $\mathcal X$ of $X$ is uncountable and real, then $X$ is a \emph{continuous} random variable. In this case there might exist a non-negative Lebesgue-integrable function $f$ such that:
$$
F_X(x) = \int_{\infty}^x f(t) dt,
$$
called the \emph{probability density function} of $X$.\\


Usually, when it comes to applying these concepts to a real problem, we will be looking at multiple variables. We would like to have a collection of random variables each one representing one of this variables.
In order to set the notation for these kinds of situations, we will introduce \emph{random vectors}.

\begin{ndef}
  A random vector is a row vector $\rvc$ whose components are rea-valued random variables on the same probabilty space $(\Omega,\Alg,P)$.
\end{ndef}

The probability distribution of a random variable can be extended in to the \emph{joint probability distribution} of a random vector.

\begin{ndef}
Let $\rvc$ be a random vector. The \emph{cumulative distribution funcion} $F_{\rv} : \R^n \to [0,1]$ of $\rv$ is defined as:
$$
F_{\rv}(x) = P(X_1 \leq x_1 , \dots, X_n \leq x_n)
$$
\end{ndef}

The distribution of each of the component random variables $X_i$ of $\rv$ are called \emph{marginal distributions}.

We would also like to know what are the most probably values that we can obtain out of a random variable.  This is called the \emph{expectation} of a random variable.

\begin{ndef}[Expectation of a \emph{R.V.}]
Let $X$ be a non negative random variable on a probability space $(\Omega,\Alg,\Prob)$. The expectation $E[X]$ of $X$ is defined as:
$$
E[X] = \int_\Omega X(\omega) \ dP(\omega)
$$
\end{ndef}
If $X$ is generic \emph{R.V}, the expectation is defined as:
$$
E[X] = E[X^+] - E[X^-]
$$
where $X^+,X^-$ are defined as it follows:
$$
X^+(\omega) = \max(X(\omega),0) \quad \quad  \quad \quad X^-(\omega) = \min(X(\omega),0)
$$

The \emph{expectation} $E[X]$ of a \emph{random variable} is a linear operation. That is, if $\mathcal Y$ is another random variable, and $\alpha,\beta \in \R$, then
$$
E[\alpha X + \beta \mathcal Y] = \alpha E[X] + \beta E[\mathcal Y]
$$
this is a trivial consequence of the linearity of the \emph{Lebesgue integral}.

As a note, if $X$ is a \emph{discrete} random variable and $\X$ is its image, its expectation can be computed as:
$$
E[X] = \sum_{x \in \X} x  P_X(x)
$$
where $x$ is each possible outcome of the experiment, and $P_X(x)$ the probability under the distribution of $X$ of the outcome $x$. The expression given in the definition before generalizes this particular case.

Using the definition of the \emph{expectation} of a random variable, we can approach to the \emph{moments} of a random variable.

\begin{ndef}
If $k \in \N$, then $E[X^k]$ is called the $k-th$ moment of $X$.
\end{ndef}
If we take $k = 1$, we have the definition of the \emph{expectation}. It is sometimes written as $m_X = E[X]$, and called the \emph{mean}. We use the \emph{mean} in the definition of the variance:

\begin{ndef}
Let $X$ be a random variable. If $E[X^2] < \infty$, then the \emph{variance} of $X$ is defined to be
$$
\Var(X) = E[(X - m_X)^2] = E[X^2] - m_X^2 
$$
\end{ndef}

Thanks to the linearity of the \emph{expectation} of a random variable, it is easy to see that
$$
Var(aX + b) = E[(aX + b) - E[aX + b])^2] = a^2E[(X - m_X)^2] = a^2 \Var(X)
$$

