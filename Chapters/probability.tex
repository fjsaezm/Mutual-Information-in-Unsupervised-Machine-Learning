
Underneath each experiment involving any grade of uncertainty there is a \emph{random variable}. This is no more than a \emph{measurable} function between two \emph{measurable spaces}.
A probability space is composed by three elements: $(\Omega, \Alg, \Prob)$. We will define those concepts one by one.

\begin{ndef}Let $\Omega$ be a non empty sample space. $\mathcal A$ is a $\sigma-$algebra over $\Omega$ if its a family of subsets of $\mathcal U$ that verify:
\begin{itemize}
  \item $\emptyset \in \Alg$
  \item If $A \in \Alg$, then $\Omega \textbackslash A \in \Alg$
  \item If $\{A_i\}_{i \in \mathbb N} \in A$ is a numerable family of $\Alg$ subsets, then $\cup_{i \in \mathbb N} A_i \in \Alg$
\end{itemize}
\end{ndef}


The pair $(\Omega,\Alg)$ is called a \emph{measurable space} To get to our probability space, we need to define a \emph{measure} on the \emph{measurable space}.

\begin{ndef}
Given $(\Omega,\Alg)$, a measurable space, a \emph{measure} $\Prob$ is a countable additive, non-negative set function on this space. That is: $\Prob: \Alg \to \mathbb R_0^+$ satisfying:
\begin{itemize}
  \item $\Prob(A) \geq \Prob(\emptyset) = 0$ for all $A \in \Alg$
  \item $P(\cup_n A_n) = \sum_n P(A_n)$ for any countable collection of disjoints sets $A_n \in \Alg$.
\end{itemize}
\end{ndef}

If $\Prob(\Omega) = 1$, $\Prob$ is a \emph{probability measure} or simply a \emph{probability}. With the concepts that have just been explained, we get to the following definition:

\begin{ndef}
A \emph{measure space} is the tuple $(\Omega, \Alg,\Prob)$ where $\Prob$ is a \emph{measure} on $(\Omega, \Alg)$. If $\Prob$ is a \emph{probability measure} $(\Omega,\Alg,\Prob)$ will be called a \emph{probability space}.
\end{ndef}

During the document, we will be always in the case where $\Prob$ is a probability measure, so we will always be talking about probability spaces. Some notation for these measures must be introduced.

An \emph{event} $A$ is a subset of the sample space $\Omega$. We define the probability of the subset as the sum of the probabilities of each of its points. That is:
$$
P[A] = \sum_{a \in A}P(a)
$$
Now, if $B$ is another event, we will use the notation $P[A,B]$ to refer to the probability of the intersection of the events $A$ and $B$, that is: $P[A,B] := P[A\cap B]$. We remark the next definition since it will be important.

\begin{ndef}
Let $A,B$ be two events in $\Omega$. The \emph{conditional probability} of $B$ given $A$ is defined as:
$$
P[B|A] = \frac{P[A,B]}{P[A]}
$$
\end{ndef}


We are ready to introduce \emph{random variables}. The first property \emph{random variable} is to be a measurable function. Those kind of functions are defined as it follows:

\begin{ndef}
Let $(\Omega_1, \Alg),(\Omega_2, \mathcal B)$ be measurable spaces. A function $f: \Omega_1 \to \Omega_2$ is said to be \emph{measurable} if, $f^{-1}(B) \in A$ for every $B \in \mathcal B$.
\end{ndef}

If $f,g$ are real-valued measurable functions, and $k \in \mathbb R$, it is true that $kf$, $f+g$ , $fg$ and $f/g$ (if $g \ne 0$) are also \emph{measurable functions}.



We are now ready to define one of the concepts that will lead us to the main objective of this thesis.

\begin{ndef}{Random Variable}
Let $(\Omega,\Alg,\Prob)$ be a probability space, and $(E,\mu)$ be a measurable space. 
A \emph{random variable} is a measurable function $\X: \Omega \to E$, from the probability space to the measurable space. This means: for every subset $B \in (E,\mu)$, its preimage
$$
\X^{-1}(B) = \{\omega : \X(\omega) \in B\} \in \Alg
$$
\end{ndef}

Using that sums, products and quotients of measurable functions are measurable functions, we obtain that \emph{sums, products and quotients of random variables are random variables}

The \emph{probability} of $\X$ taking a concrete value on a measurable set contained in $E$, say, $S \in E$, is written as:
$$
P_X(S) = P(X \in S) = P(\{a \in \Omega : X(a) \in S\})
$$

A very simple example of random variable is the following:

\begin{nexample}
  Consider tossing a coin. The possible outcomes of this experiment are \emph{Heads or Tails}. Those are our random events. We can give our random events a possible value. For instance, let \emph{Heads} be $1$ and \emph{Tails} be 0. Then, our random variable looks like this:
  \begin{equation*}
      \mathcal \X  = \left\{ \begin{aligned}
  1 \quad \text{if we obtain Heads} \\
  0 \quad \text{if we obtain Tails}
\end{aligned}\right.
  \end{equation*}

\end{nexample}

In the last example, our random variable is \emph{discrete}, since the set $\{\X(\omega): \omega \in \Omega\}$ is finite. A \emph{Random Variable} can also be \emph{continuous}, if it can take any value within an interval.
We will refer to a \emph{Random Variable} as a \emph{R.V.}. \\

\begin{ndef}
The \emph{distribution function } $F_\X$ of a real-valued random variable $\X$ is:
$$
F_\X(x) = P(\X \leq x) = P(\{\omega : \X(\omega) \leq x\}) = P_\X((-\infty,x]) \quad \forall x \in \mathbb R
$$
\end{ndef}

And now, it will be introduced one of the most important concepts when talking about a \emph{R.V.}, its expectation.

\begin{ndef}[Expectation of a \emph{R.V.}]
Let $\X$ be a non negative random variable on a probability space $(\Omega,\Alg,\Prob)$. The expectation $E[\X]$ of $\X$ is defined as:
$$
E[\X] = \int_\Omega \X(\omega) \ dP(\omega)
$$
\end{ndef}
If $\X$ is generic \emph{R.V}, the expectation is defined as:
$$
E[\X] = E[\X^+] - E[\X^-]
$$
where $\X^+,\X^-$ are defined as it follows:
$$
\X^+(\omega) = max(\X(\omega),0) \quad \quad , \quad \quad \X^-(\omega) = min(\X(\omega),0)
$$

The \emph{expectation} $E[\X]$ of a \emph{random variable} is a linear operation. That is, if $\mathcal Y$ is another random variable, and $\alpha,\beta \in \R$, then
$$
E[\alpha \X + \beta \mathcal Y] = \alpha E[\X] + \beta E[\mathcal Y]
$$
this is a trivial consequence of the linearity of the \emph{Lebesgue integral}.

As a note, if $\X$ is a \emph{discrete} random variable, its expectation can be computed as:
$$
E[\X] = \sum_{i = 0}^n x_i p_i
$$
where $x_i$ is each possible outcome of the experiment, and $p_i$ the probability of the outcome $x_i$. The expression given in the definition before generalizes this particular case.

Using the definition of the \emph{expectation} of a random variable, we can approach to the \emph{moments} of a random variable.

\begin{ndef}
If $k \in \N$, then $E[\X^k]$ is called the $k-th$ moment of $\X$.
\end{ndef}
If we take $k = 1$, we have the definition of the \emph{expectation}. It is sometimes written as $m_\X = E[\X]$, and called the \emph{mean}. Using  \emph{the mean} and we get to the definition of the \emph{variance}:

\begin{ndef}
Let $\X$ be a random variable. If $E[\X^2] < \infty$, then the \emph{variance} of $\X$ is defined to be
$$
Var(\X) = E[(\X - m_\X)^2] = E[\X^2] - m_\X^2 
$$
\end{ndef}

Thanks to the linearity of the \emph{expectation} of a random variable, it is easy to see that
$$
Var(a\X + b) = E[(a\X + b) - E[a\X + b])^2] = a^2E[(\X - m_\X)^2] = a^2 Var(\X)
$$

