
\label{Chapter:MI}
Using the entropy of a random variable we can directly state the definition of \emph{mutual information} as follows:

\begin{ndefC}
Let $X,Z$ be random variables. The \emph{mutual information (MI)} between $X$ and $Z$ is expressed as the difference between the entropy of $X$ and the conditional entropy of $X$ and $Z$, that is:
$$
I(X,Z) := H(X) - H(X|Z).
$$
\end{ndefC}
Since the entropy of the random variable $H(X)$ explains the uncertainty of $X$ occurring, the intuitive idea of the \emph{MI} is to determine the decrease of uncertainty of $X$ occurring when we already
know that $Z$ has occurred. We also have to note that, using the definition of the \emph{entropy} and the same argument that we used to obtain the expression in Eq. \eqref{eq:dif-expr-mi}, we can rewrite the \emph{MI}  it follows:
\begin{align}
I(X,Z) & = \sum_{x \in \X}P_X(x) \log \frac{1}{P(x)} - \sum_{x \in \X, z \in \mathcal Z} P_{XZ}(x,z) \log \frac{P_Z(x)}{P_{XZ}(x,z)} \nonumber \\
 & = \sum_{x,z}P_{XZ}\log \frac{P_{XZ}(x,z)}{P_Z(z)P_X(x)} \label{MI:sum:xz}
\end{align}
and if we compare it to the formula of the KL-Divergence, we obtain:
\[
I(X,Z)  = \sum_{x,z}P_{XZ}\log \frac{P_{XZ}(x,z)}{P_Z(z)P_X(x)} = D_{KL}(P_{XZ} \ || \ P_X P_Z),
\]
so we have obtained an expression of the mutual information using the \emph{Kullback-Leibler} divergence. This provides with the following immediate consequences:

\begin{corollary}

\begin{enumerate}[label=$(\roman*)$]
\item Mutual information is non-negative. That is : $I(X,Z) \geq 0$.
\item If $X,Z$ are random variables, then its mutual information equals zero if, and only if, they are independent. 

\item Mutual information is symmetric. That is: $I(X,Z) = I(Z,X)$.
\end{enumerate}
\end{corollary}

\begin{proof}
    \begin{enumerate}[label=$(\roman*)$]
        \item This is trivial using Prop \ref{entr:prop:2} and the definition of the mutual information.
        \item We can use the KL-Divergence formulation to see that since $$
        D_{KL}(P_{XZ} \ || \ P_X P_Z) = 0 \implies P_{XZ} = P_X P_Z,
        $$ 
        almost everywhere then $X$ and $Z$ are independent.
        \item It is a consequence of the fact that $P_{XZ} = P_{ZX}$ and $P_X P_Z = P_Z P_X$.
        \end{enumerate}
\end{proof}

Later in this document, we will have some sort of random variable $X$ and would like it to maintain the mutual information with itself after being applied a function. The following proposition will be useful:

\begin{nprop}
Let $X,Z$ be random variables. Then, $I(X,Z)$ is invariant under homeomorphism.
\end{nprop}
\begin{proof}
Let $\phi(x)$ be an homeomorphism, i.e., a continuous, monotonic function with $\phi^{-1}(x)$ also continuous and monotonic. Let $X$ be a random variable and $Y$ another one such $y = \phi(x)$ if $x = X(\omega)$ for some $\omega \in \Omega$. Then, if $S$ is a particular subset we have 
\[
P(Y \in S) = \int_S P_Y(y) dy = \int_{\phi^{-1}(S)}P_X(x) dx \stackrel{(1)}{=} \int_S P_X(\phi^{-1}(y)) \abs{ \frac{d \phi^{-1}}{dy}}dy,
\]
where in $(1)$ we have changed from $x$ to $y$. Hence, 
\[
P_Y(y) = P_X(\phi^{-1}(y))\abs{\frac{d \phi^{-1}}{dy}}.
\]
As a consequence of this, $I(X,Z) = I(\phi(X),Z) $ for any homeomorphism $\phi$. By symmetry, the same holds for $Z$.

\end{proof}

\begin{remark} We can set a connection between the mutual information and sufficient statistics. Let $T(X)$ be a statistic. We say that $T(X)$  is sufficient for $\theta$ if its mutual information with $\theta$ equals the mutual information between $X$ and $\theta$, that is:
$$
I(\theta, X) = I (\theta, T(X)).
$$
This means that sufficient statistics preserve mutual information and conversely.
\end{remark}

\subsection{Lower bounds on Mutual Information}

Although mutual information seems like a relatively intuitive concept, it is most of the times extremely hard to compute it in real life problems in which the distributions $P(x,z),P(x),P(z)$ are not known.
\begin{nexample}
Let $x$ represent an image of size $n \times m$ pixels. Then, the dimension of the single image is $n \cdot m \cdot 3$, for RGB color channels. In these cases, there is no easy way of calculating $P(x)$.
\end{nexample}

Due to this problem related to the \emph{Curse of Dimensionality}, we can try to compute lower bounds of it that are generally easier to calculate. We will now present two general lower bounds, and the third bound will be proved later in Chapter \ref{Chapter:CPC}.

\subsubsection{Variational Lower Bound}



Using the expression of the mutual information in terms of entropy, $I(x,z) = H(z) - H(z|x)$, we can give a lower bound on $I(x,z)$ as a function of a probability distribution $Q_\theta(z|x)$. 

\begin{nprop}
Let $X,Z$ be random variables and $Q_\theta(z|x)$ be an arbitrary probability distribution. Then,
$$
I(x,z) \geq H(z) + E_{P_X} \left[ E_{P_{X|Z}}\left[\log Q_\theta(z|x)\right]\right] 
$$
\end{nprop}

\begin{proof}
Recalling that
$$
H(z|x) = - E_{P_{XZ}} \left[ \log P(x,z) - \log P(x)\right],
$$
and that
\begin{align*}
E_{P(x,z)}\left[\log\frac{P(x,z)}{P(x)}\right] & =  \sum_{x,z} P(x,z) \log\frac{P(x,z)}{P(x)} \\ 
& = \sum_{x,z} P(x)P(z|x) \log P(z|x) = \sum_{x,z} P(x) E_{P(z|x)}[\log P(z|x)]\\
 & =  E_{P(x)}\left[E_{P(z|x)}[\log P(z|x)]\right],
\end{align*}
we only have to use the definition of the conditional probability to see that:
\begin{align*}
I(x,z) & =  H(z) - H(z|x) \\
    & =  H(z) + E_{P(x,z)} = H(z) + E_{P(x,z)} \left[ \log \frac{P(x,z)}{P(x)}\right] \\
    & =  H(z) + E_{P(x)} \left[ E_{P(x|z)}\left[\log P(z|x)\right]\right] \\
    & = H(z) + E_{P(x)} \left[ E_{P(x|z)} \left[\log \frac{P(z|x)}{Q_\theta(z|x)}\right] + E_{P(z|x)}\left[\log Q_\theta(z|x)\right]\right] \\
    & =  H(z) + E_{P(x)}\left[ \underbrace{D_{KL}(P(z|x)||Q_\theta(z|x))}_{\geq 0} + E_{P(z|x)}\left[\log Q_\theta(z|x)\right] \right]\\
    & \geq H(z) + E_{P(x)}\left[E_{P(z|x)}\left[ \log Q_\theta(z|x)\right]\right].
\end{align*}
We have taken advantage of the non-negativity of the KL-Divergence.
\end{proof}

Using this bound, and combining this theoretical knowledge with machine learning methods, such as \emph{backpropagation}, we can make $Q_\theta$ be a neural network and maximize this lower bound.

\subsubsection{Donsker-Varadhan Representation lower bound}

We can also give a lower bound on the mutual information using its KL-Divergence formulation. Firstly, we have to 

\begin{nth}[Donsker-Varadhan]
The KL divergence admits the following dual representation:
\[
D_{KL}(P || Q) = \sup_{T} E_P[T] - \log E_Q\left[e^T\right],
\]
where the supreme is taken over all functions $T:\Omega \to \R$ such that both expectations exist.
\end{nth}
\begin{proof}
    Let $T$ be a given function. We can consider the \emph{Gibbs} distribution, which is defined by
    \[
    G(x) = \frac{1}{Z}e^T Q(x),
    \]
    where $Z$ is a normalization term defined by $Z = E_{Q(x)}[e^T]$. We observe by the definition of $G$ that, taking logarithms and then taking expectations respect to $P(x)$, we obtain
    \begin{equation}\label{alternative:gibbs}
    E_{P}\left[ \log \frac{G(x)}{Q(x)}\right] = E_{P}[T] - \log Z = E_P[T] - \log \left( E_Q\left[e^T\right]\right)
    \end{equation}
    Let $\Delta$ be the gap between $D_{KL}$ between the distributions $P$ and $Q$, and the RHS of the Equation \eqref{alternative:gibbs},
    \[
    \Delta := D_{KL}(P||Q) - \left( E_{P}[T] - \log(E_Q\left[e^T\right])\right).
    \]
    Applying the definition of KL-Divergence and Equation \eqref{alternative:gibbs}, we obtain
    \begin{align}
    \Delta & = E_{P}\left[\log \frac{P}{Q}\right] - E_{P}\left[ \log \frac{G(x)}{Q(x)}\right] \nonumber \\
    & = E_P \left[\log \frac{P}{Q} - \log \frac{G}{Q}\right] \nonumber \\
    & = E_P\left[\log \frac{P}{G}\right] \nonumber \\
    & = D_{KL}(P||G) \label{delta:DKL}.
    \end{align}
    Hence, since $D_{KL}(P||G) \geq 0$ for any distributions $P,G$, we have that $\Delta \geq 0$ and therefore
    \[
    D_{KL}(P||Q) \geq E_P[T] - \log \left(E_Q\left[e^T\right]\right). 
    \]
    This inequality is preserved upon taking the supreme over the right hand side. Also, Equation \eqref{delta:DKL} shows that the bound is \emph{tight} whenever $P(x) = Q(x)$ for all $x$, namely for optimal functions $T^*$ that have the form
    \[
    T^* = \log \frac{P(x)}{Q(x)} + C,    
    \]
    for some $C \in \R$.
    
\end{proof}

Using this representation, we reach this lower bound as a corollary. 

\begin{corollary}
    Let $\mathcal F$ be any class of functions $T: \Omega \to \R$ satisfying the integrability constraints of the theorem. Then, 
$$
I(P,Q) = D_{KL}(P||Q) \geq \sup_{T \in \mathcal F} E_P[T] - \log E_Q\left[e^T\right].
$$
\end{corollary}

This bound may seem a bit odd in the context of Machine Learning. However, it is very useful. It was used in \cite{belghazi2018mine} to estimate the mutual information between two random variables using neural networks.

Furthermore, since it is a lower bound, it can be used as a \emph{loss function} to learn representations. In \cite{hjelm_learning_2019}, $T_\theta$ parametrized a neural network that acted as a discriminator between samples drawn from the joint distribution $P(x,z)$ and samples from the marginal distribution $P(x)P(z)$. They discovered that the inputs to the neural network, which were representations of the original data, must have high mutual information with the original data in order to obtain good results.
