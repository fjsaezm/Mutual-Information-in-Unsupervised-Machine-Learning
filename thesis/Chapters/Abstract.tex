\emph{Representation learning} is the process of, given an input data of any type, training a model that learns to produce a lower dimensional vector that summarizes the information contained in the input. In this document, we will study the main methods used nowadays in this field.

Firstly, an introduction to the basic probability concepts is made, along with an introduction on the contrastive noise estimation problem, which will be key in the construction of the \emph{InfoNCE} loss and great inspiration for the contrastive methods that currently achieve the state of art results in this field.

In this context, \emph{mutual information} appears as a good method to obtain good representations by maximizing this function between the input of the model and the representation obtained. We review the most important properties of this measure and, since it is hard to explicitly compute it, we present some lower bounds on this function which can make the task easier.

Lastly, a few paper empirically showed that minimizing mutual information is not the best way of obtaining good representations for downstream tasks. New frameworks (SimCLR, BYOL) that present different perspectives of the same input to the models have shown to perform better, so we present these frameworks and review what choices have to be explored in order to achieve the best performance using these kind of methods.




\textbf{Keywords:} \emph{representation learning, noise contrastive estimation, entropy, mutual information, lower bounds, siamese networks, triplet loss} and \emph{deep learning}.