
\label{Chapter:CPC}
We are now ready to connect the concepts of mutual information and generative models that we have presented. In unsupervised learning,
it is a common strategy to predict future information and to try to find out if our predictions are correct.
In \emph{natural language processing}, for instance, representations are learned 
using neighbouring words \citep{mikolov_efficient_2013}. In the field of computer vision, some studies have been able to predict color from grayscale \citep{doersch_unsupervised_2016}.

When we work with high-dimensional data, it is not realistic to make use of an unimodal loss function to evaluate our model. If we did it like this, we would be assuming that there is only
one peak in the distribution function and that it is actually similar to a Gaussian.  This is not always true, so we can not assume it for our models. Generative models can be used for this purpose:
these models can be used to model the relationships in the data. However, they ignore the context $c$ in which the data $x$ is involved. As an easy example of this, an image contains thousands of bits of information,
while the label that classifies the image contains much less information , say, $10$ bits for $1024$ categories. Because of this, modeling $P(x|c)$ might not be the best way to proceed if we want
to obtain the real distribution that generates our data. 

During the last few years, the representation learning problem has been approached using different machine learning frameworks. The most competitive ones have been self-supervised contrastive representation learning \cite{oord_representation_2019,tian_what_2020,hjelm_learning_2019,gutmann_noise-contrastive_nodate,chen_simple_2020,he_momentum_2020} using \emph{contrastive losses}, and they have empirically outperformed other approaches.     

In contrastive learning, different ``views'' of the same input are created. These are also called \emph{positive samples}. Then, they are compared with \emph{negative samples}, which are views created from an input that does not share information with the input of the positive sample. A very interesting idea would be to try and maximize the mutual information between positive samples and push apart the views taken from negative samples. 

There are many ways of creating samples, both positive and negative, of an input. Usually, this is done by using data augmentation using the available images. Thus, we can use any of the presented techniques before to create a positive sample. A negative sample is created by using any data augmentation technique in an image that belongs to a different class.

In fact, if $v_1,v_2$ are two views of an input, we can think of the positive pairs as points coming from a joint distribution over the views $P(v_1,v_2)$, and negative samples coming from the product of the marginals $P(v_1)P(v_2)$, \citep{tian_what_2020}.

It is important to find a way to determine how much shared information between the views is needed, in order to make the representations obtained good enough for any downstream task. Here is where the \emph{InfoMin principle} is born. 

\begin{ndefC}[The InfoMin principle]\label{def:infomin}
    A good set of views are those that share the minimal information necessary to perform well at the downstream task.
\end{ndefC} 


Our goal here will be to seek for a way of extracting shared information between the context $c$ and the data $x$. Here is where we link the mutual information with representation learning. Remember that the mutual information of two random variables, say $x$ and $c$ in this case, is:
\begin{equation}\label{EQ:MI}
I(x,c) = \sum_{x,c}P(x,c)\log\frac{P(x|c)}{P(x)}
\end{equation}
Maximizing the MI between $x$ and $c$, we extract the latent variables that the inputs have in common. 

\section{Contrastive Predictive Coding}

We can apply these concepts in a concrete framework, presented firstly in \cite{oord_representation_2019}. Let us see what information is used and how it is treated in order to train a model that tries to obtain useful representations for downstream tasks.

In this section, if $x$ is an input signal for our network, $x_t$ will be the value of the input at instant $t$. We will also make reference to $\xtk$, meaning that $\xtk$ is $k$ steps ahead of time to $x_t$

Firstly, an \emph{encoder} is used. An encoder is a model that, given an input $x$, provides a feature map or vector that holds the information that the input $x$ had. We will use an encoder $g_{enc}$ that transforms the input sequence of observations $x_t$ to a sequence of latent representations
$$
z_t = g_{enc}(x_t).
$$
After we have obtained $z_t$, we use it as input of an autoregressive model, explained before, to produce a context latent representation:
$$
c_t = g_{ar}(z_{\leq t}).
$$
In this case, $c_t$ will summarize the information of $z_i$ for $i \leq t$. Following the argument that we gave before, predicting the future $\xtk$ using only a generative model (say $P_k(\xtk|c)$)
might not be correct, since we would be ignoring the context. \\

\begin{figure}[H]
    \centering 
    \includegraphics[scale=0.4]{contrastive_repr4.pdf}
    \caption{Image from \citep{oord_representation_2019}. Overview of Contrastive Predictive Coding framework using audio signal as input. }
\end{figure}

Let us see how we train the encoder $g_{enc}$ and the autoregressive model $g_{ar}$.

In Chapter \ref{Chapter:NCE}, we gave the notions of the general idea of Noise Contrastive Estimation. Now, we can apply those ideas to a particular case, in which one of the subsets, say $X$ only has one element, and the other one has $N-1$ elements. We combine both sets in $X$ for the following argument.

Let $X = \{x_1,\dots,x_N\}$ be a set of $N$ random samples. $X$ will contain a positive sample taken from the distribution $P( \xtk |c_t)$ and $N-1$ negative
samples from the distribution proposed $P(\xtk)$. With this set, we would like to optimize the following loss function, which is an alternative expression of Equation \eqref{log:likelihood:red}:
\begin{ndef}
The loss $\mathcal L_N$ defined as
\begin{equation}\label{NCE:loss}
\mathcal L_N = - \underset{X}{E} \left[\log \frac{f_k(\xtk,c_t)}{\sum_{x_j \in X} f_k(x_j,c_t)}\right],
\end{equation}
is known as the \emph{InfoNCE} (Information Noise Contrastive Estimation) loss. We have defined it this time for the particular case of this problem, but changing $f_k$ for a function depending on some parameters and $\xtk$ and $c_t$ for positive and negative samples, this loss is generalized to a contrastive loss for any contrastive problem.

This loss also gives the bound in Proposition \ref{Prop:Bound:NCE} the name of \emph{InfoNCE Bound}.

\end{ndef}

Let us have a look at the \emph{categorical cross-entropy} loss function:
\[
    \mathcal L(y,s) = -\sum_i^C y_i \log(s_i)    
\]
where $C$ is the number of possible classes in a classification problem, $y_i$ are the groundtruth of each class and $s_i$ is the score of each class.

As we remarked before, we can say that $\mathcal L_N$ is no more than the categorical cross-entropy of classifying the positive sample of $X$ correctly, with the argument of the logarithm being the prediction
of the model. If we note with $[d = i]$ as an indicator of the sample $x_i$ being the positive sample in $X$, the optimal probability for this loss can be written as $P(d = i|X,c_t)$. It is now clear that this loss is based in Noise Contrastive Estimation \citep{gutmann_noise-contrastive_nodate}.

Now, the probability that $x_i$ was drawn from the conditional distribution $P(\xtk|c_t)$ that has the context in account, rather than the proposal distribution $P(\xtk)$ that does not have $c_t$ in account, leads us to the following expression:

$$
P(d = i | X , c_t) = \frac{ \frac{P(x_i|c_t)}{P(x_i)}}{\sum_{j=1}^N \frac{P(x_j|c_t)}{P(x_j)}}.
$$

This is the optimal case for Equation \eqref{NCE:loss}.



In fact, if we denote $f(\xtk,c_t)$  as the density ratio that preserves the mutual information between $\xtk$ and $c_t$ in the mutual information definition \eqref{EQ:MI}, if $x_{t+k}$ is $k$ steps ahead on time respect to $x_t$, then
\begin{equation}\label{EQ:Proportional}
f_k(\xtk,c_t)  \propto \frac{P(x_{t+k}| c_t)}{P(\xtk)},
\end{equation}
where $\propto$ means that the member on the left is proportional to the member on the right. We can see that the optimal value $f_k(\xtk,c_t)$ does not depend on $N-1$, the number of negative samples in $X$.
Using this density ratio, we are relieved from modeling the high dimensional distribution $x_{t_k}$. In \cite{oord_representation_2019}, for instance, the following log-bilinear model expression is used:
$$
f_k(\xtk,c_t) = exp(z_{t+k}^T W_k c_t).
$$


In the proposed model, we can either use the representation given by the encoder $(z_t)$ or the representation given by the autoregressive model $(c_t)$ for downstream tasks. Clearly, the representation that aggregates information from past inputs will be more useful
if more information about the context is needed. Furthermore, any type of models for the encoder and the autoregressive models can be used in this kind of framework.



\subsection{Contrastive Lower Bound}

In the context of this framework, a new lower bound for the mutual information was born. This lower bound uses the contrastive loss function and the number of elements used in the noise contrastive estimation. Let us present the bound in general.

Let $(x,z)$ be a data representation drawn from a distribution $P(x,z)$ and $x'$ be some other data drawn from the distribution $P(x)$. Using NCE, we should be able to say that $(x,z)$ was drawn from the distribution $P(x,z)$ (which was $P_d$ in the NCE theory) while $(x',z)$ was drawn from the product of the marginal distributions $P(x)P(z)$ (which was $P_n$ in the explanation of NCE). Let $h_\theta$ be a model that helps us to do this discrimination, with parameters $\theta$. 

As we did before, we want to estimate the ratio $P_d/P_n$ of the different distributions, in this case the ratio would be $P(x,z)/P(x)P(z)$. Let $(x^*,z)$ be a pair drawn from $P(x,z)$ and $X = \{x^*, x_1,\cdots,x_{N-1} \}$, where the rest of the $N-1$ points form pairs $(x_j,z)$ drawn from $P(x)P(z)$ the product of the marginal distribution. We can rewrite the loss in Equation \eqref{log:likelihood:red} in a simpler expression:
\begin{equation}\label{log:likelihood:rewritten}
l(\theta) = - E_X \left[ \log \frac{h_\theta(x^*,z)}{\sum_{x \in X}h_\theta(x,z)}\right]  .
\end{equation}

If we maximize this objective, $h_\theta$ learns to discriminate $(x^*,z)$ from $(x_j,z)$ for $ 1 \leq j < N$ and, thus, we are learning to estimate the ratio $P(x,z)/P(x)P(z)$. Let us see how maximizing $\ell(\theta)$ we are maximizing a lower bound for $I(x,z)$.

\begin{nprop}\label{Prop:Bound:NCE}
Let $X = \{x^*, x_1,\cdots,x_{N-1} \}$, where $x^* \sim P(x,z)$ and the rest of them were sampled from $P(x)P(z)$. Then,
\begin{equation}\label{Bound:NCE}
I(x,z) \geq -  \ell(\theta) + \log N
\end{equation}
\end{nprop}
\begin{proof}

Firstly, using Bayes' rule, $P(x^*,z) = P(x^*|z)P(z)$. Hence, since $h_\theta$ estimates $P(x^*,z)/P(x)P(z)$, it also estimates
\[
\frac{P(x^*,z)}{P(x)P(z)} = \frac{P(x^*|z)P(z)}{P(x)P(z)} = \frac{P(x^*|z)}{P(x)}.
\]
Using the definition of the log-likelihood that we see in Equation \eqref{log:likelihood:rewritten}, forgetting the sign for the moment, we see that
\begin{align*}
E_X \left[ \log \frac{h_\theta(x^*,z)}{\sum_{x \in X}h_\theta(x,z)}\right] & =  E_X \left[ \log \frac{h_\theta(x^*,z)}{ h_\theta(x^*,z) + \sum_{j = 1}^{N-1} h_\theta(x_j,z)}\right] \\
& \approx E_X \left[ \log \frac{\frac{P(x^*|z)}{P(x)}}{ \frac{P(x^*|z)}{P(x)} + \sum_{j = 1}^{N-1} \frac{P(x_j|z)}{P(x)}}\right].
\end{align*}
Now, using that $\log(a) = -log(a^{-1})$,
\begin{align*}
E_X \left[ \log \frac{\frac{P(x^*|z)}{P(x)}}{ \frac{P(x^*|z)}{P(x)} + \sum_{j = 1}^{N-1} \frac{P(x_j|z)}{P(x)}}\right] & = E_X\left[ -\log\left( \frac{\frac{P(x^*|z)}{P(x)} + \sum_{j = 1}^{N-1} \frac{P(x_j|z)}{P(x)}}{\frac{P(x^*|z)}{P(x)}}\right) \right] \\
& = E_X\left[ -\log\left( 1+ \frac{ \sum_{j = 1}^{N-1} \frac{P(x_j|z)}{P(x)}}{\frac{P(x^*|z)}{P(x)}} \right) \right] \\
& = E_X\left[ -\log\left( 1+ \frac{ (N-1) E_{X - \{x^*\}}\left[\frac{P(x|z)}{P(x)}\right] }{\frac{P(x^*|z)}{P(x)}}\right)\right] \\
\end{align*}
Now, since $E_{X - \{x^*\}} \left[ \frac{P(x|z)}{P(x)} \right] = \sum_{x_j \in X - \{x^*\}} P(x_j) \frac{P(x_j|z)}{P(x_j)} = 1$, then
\[
E_X\left[ -\log\left( 1+ \frac{ (N-1) E_{X - \{x^*\}}\left[\frac{P(x|z)}{P(x)}\right] }{\frac{P(x^*|z)}{P(x)}}\right)\right]  
= E_X\left[ -\log\left( 1+ \frac{ (N-1)}{\frac{P(x^*|z)}{P(x)}} \right)\right].
\] 
Lastly, using that if $k > 0$, then $- \log a(k+1) \geq -\log(1+ak)$, we obtain:
\begin{align*}
E_X\left[ -\log\left( 1+ \frac{ (N-1)}{\frac{P(x^*|z)} {P(x)}} \right)\right] &  =
E_X\left[ \log \frac{1}{ 1+ \frac{P(x^*)}{P(x^*|z)}(N-1)}\right] \\
& \leq E_X\left[ \log\left(\frac{1}{\frac{P(x^*)}{P(x^*|z)}}\frac{1}{N}\right)\right]\\
& = E_X\left[ \log\left(\frac{P(x^*|z)}{P(x^*)} \frac{1}{N}\right)\right]\\
& = E_X \left[ \log\left(\frac{P(x^*|z)}{P(x^*)}\right)\right] - \log N \\
& \stackrel{(1)}{=} E_X \left[ \log\left(\frac{P(x^*,z)}{P(x^*)P(z)}\right)\right] - \log N\\
& \stackrel{(2)}{=} I(x,z) - \log N,
\end{align*}
where, in $(1)$, we have use Bayes' rule again and in $(2)$ we have used the definition of the MI that we found in equation \eqref{MI:sum:xz}. Looking at the first and last equations used in this proof, and seeing that we have $\leq$ in the middle of the chain of equalities, we have proved
\[
    E_X \left[ \log \frac{h_\theta(x^*,z)}{\sum_{x \in X}h_\theta(x,z)}\right] = - \ell (\theta) \leq I(x,z) - \log N,
\]
which implies 
\[
I(x,z) \geq -\ell(\theta)+ \log N  
\]
as we wanted to see.   
\end{proof}


\section{Good views for Contrastive Learning}

We have presented a framework in which a set $X = \{x_1,\cdots,x_n\}$ contains a sample from the distribution $P(\xtk,c_t)$ and the rest are samples from the distribution $P(\xtk)$. This samples are different views of the data. 

The choice of the views affects the results in the downstream tasks \citep{tian_what_2020}. To clarify, the views will affect on the training, hence, it will affect to the representations that are obtained. We would like to have some guarantees that the views that we are choosing provide us with good examples for our training. Let us formalize this idea.

Given two random variables $v_1,v_2$, our goal until now was to learn a function to discriminate the samples from the joint distribution and the product of the marginal distributions, resulting on a mutual information estimator between $v_1$ and $v_2$. In practice, $v_1$ and $v_2$ are two views of the same input $x$, using one of the methods that we mentioned in the introduction. We would like to have that, if $y$ is a downstream task, the mutual information between both the inputs and the downstream task, is the same as the mutual information between the input $x$ and the downstream task, i.e.:
\[
I(v_1,y) = I(v_2,y) = I(x,y).  
\]
Also, we would like to remove the information that is not relevant for our downstream task. This is done by obtaining the pair of views $(v_1^*,v_2^*)$ such that the mutual information between them is the minimum of the mutual information between all the possible views $(v_i,v_j)$. Formally, that is obtaining
\[
(v_1^*, v_2^*) = \min_{v_1,v_2} I (v_1,v_2),    
\]
These two ideas form the \emph{InfoMin Principle} that we mentioned before in Definition \ref{def:infomin}.

Usually, the views are encoded using an encoder $f$, not having it to be the same for both views. We can say that $z_i = f_i(v_i)$ for $i \in \{1,2\}$. If an encoder is sufficient, then it has to maintain the mutual information between the random variables after one of them has been encoded. More formally,

\begin{ndef}
We say an encoder $f_i$ of a view $v_i$, with $i \in \{1,2\}$ is \emph{sufficient} in the contrastive learning framework if, and only if it maintains the mutual information between the pairs $(v_i,v_j)$ and $(f_i(v_i),v_j)$ with $j \in \{1,2\}$. That is
\[
I(v_i,v_j) = I(f_i(v_i),v_j).    
\]
\end{ndef}
This usually means that no information was lost in the process of encoding. We want to extract only the most essential information and do not learn the ``extra'' information between the views.
\begin{ndef}
We say that a sufficient encoder $f_i$ of $v_i$ is \emph{minimal} if, and only if, the mutual information between $(f_i(v_i),v_j)$ is lesser than the mutual information between $(f(v_i),v_j)$ for any other sufficient encoder $f$. That is:
\[
I(f_i(v_i),v_j) \leq I(f(v_i),v_j) \quad \text{for all sufficient}\ \ f. 
\]
\end{ndef}

With these notions already presented, we would like to define what representations are good for a downstream task. We get to the following definition \citep{tian_what_2020}:
\begin{ndef}
For a task $\mathcal T$, whose goal is to predict a label $y$ from the input data $x$, the optimal representation $z^*$ encoded from $x$, that is $z^* = f(x)$ for some encoder $f$, is the minimal sufficient statistic with respect to $y$.
\end{ndef}
This means that if we use $z^*$ to make a prediction using a machine learning model, we are using the same information that we would be using the whole input $x$. What is more, since we are following the InfoMin principles and we are dismissing all the non relevant information, $z^*$ provides with the smallest complexity.

\begin{nprop}\label{prop:optimal:views}
Let $f_1,f_2$ be minimal sufficient encoders, and $\mathcal T$ be a downstream task with label $y$. Then, the optimal views $(v_1^*,v_2^*)$ from the data $x$ are the ones that have minimal mutual information
\[
    (v_1^*,v_2^*) = \argmin_{v_1,v_2} I(v_1,v_2),
\]
subject to to $I(v_1,y) = I(v_2,y) = I(x,y)$.

Given the optimal views $(v_1^*,v_2^*)$, the representation $z_1^*$ learned by contrastive learning is optimal for $\mathcal T$.
\end{nprop}
The proof of this proposition is out of the scope of this work so no further information will be provided. The last statement of the Proposition \ref{prop:optimal:views} is a consequence of the minimality and sufficiency of $f_1$ and $f_2$. 

This proposition carries the most important mathematical conclusion from this section. It will serve to prove sufficient conditions for views to be effective for contrastive learning, where effectiveness is measured as effectiveness in downstream tasks.


