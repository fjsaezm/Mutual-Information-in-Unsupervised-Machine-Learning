\label{Chapter:connection:triplets}

Contrastive learning exploits the idea of comparing the input $x$ with other different examples, either from the same class or from another class. It aims to produce closer representations for the examples of the same class and distant representations for elements of other classes.

We have been using the loss in Equation \eqref{NCE:loss}, which we built using noise contrastive estimation. However, this is not the only way of approaching this problem, and other types of losses have been used for similar purposes, forgetting the part of mutual information maximization and replacing it with \emph{geometrical distance} optimization.

In this section, we will present \emph{Triplet losses}, other kind of loss functions that also compare different views of the same input $x$.

\section{From deep metric learning to triplet losses and its generalization}

Distance metric learning also aims to learn an embedding representation of an input data $x$ that preserves the distance between similar data points close and also makes de distance between different datapoints far on the embedding space \citep{Sohn2016ImprovedDM}.

Let us set the notation that we will use first. We will consider sets of triplets $(x,\ps,\ns)$ where:
\begin{itemize}
\item The element $x$ is an anchor point,
\item The element $\ps$ is a positive instance,
\item The element $\ns$ is a negative instance.
\end{itemize}

\begin{nexample}
    Let us present a very simple example. If our input image is a cat, that would be the anchor $x$. Clearly, a positive instance would be an image of another cat or even the same cat seen from another perspective. A negative instance would be a photo of any other animal, in this case we use a dog.
    \begin{figure}[H]%!htb]
        \minipage{0.32\textwidth}
          \includegraphics[width=\linewidth]{media/c1}
          \caption*{Anchor}\label{fig:cat1}
        \endminipage\hfill
        \minipage{0.32\textwidth}%
          \includegraphics[width=\linewidth]{media/c2}
          \caption*{Positive example}\label{fig:c2}
        \endminipage
        \minipage{0.32\textwidth}%
          \includegraphics[width=\linewidth]{media/doggo}
          \caption*{Negative example}\label{fig:doggo}
        \endminipage
        \caption{Example of an anchor $x$, a positive instance $\ps$ and a negative instance $\ns$. Images obtained from \emph{Google}.}
        \end{figure}
    \end{nexample}
    


The main idea is to learn a representation of $x$, say $g(x)$, such that the distance of the representation of the input is closer in distance to the representation of the positive sample $\ps$ than the representation of the negative sample $\ns$. Using the norm\footnotemark, we can formally express that as follows: 
$$
\norm{g(x) - g(\ps)}_2 \leq \norm{g(x) - g(\ns)}_2,
$$
for each triplet in the set.


%------------- Footnotemark
\footnotetext{A definition of the norm can be found on Appendix \ref{APPENDIX:A}, Definition \ref{def:norm}. }
%----------------------


Support-vector machines (SVMs) are supervised learning models used for classification or regression problems. They are one of the most robust prediction methods. They search for a hyperplane $h$ in high or infinite dimensional space that separates the data as much as possible, making use of \emph{support vectors}, the datapoints that are closest to the hyperplane. If the data is linearly separable, we can select two hyperplanes $h_1,h_2$ that are parallel to $h$ and making the distance from them to $h$ as large as possible. That region is called the \emph{margin}.

Coming back to our triplets problem, we also want to introduce a margin between the distances of the elements of the triplets, in order to separate positive examples from negative examples as much as possible. This way, we introduce a \emph{margin} term $\alpha$, rewriting our last equation as follows:
\[
\norm{g(x) - g(\ps)}_2 + \alpha < \norm{g(x) - g(\ns)}_2.
\]
Using this inequality, we can define a hinge loss function for each triplet in the set:
\begin{equation}\label{triplet:single:loss}
\ell^\alpha (x,\ps,\ns) = \max \left(0, \norm{g(x) - g(\ps)}_2^2 - \norm{g(x) - g(\ns)}_2^2 + \alpha\right).
\end{equation}
This loss has been defined for a single triplet. Now, we can define a global loss that accumulates the loss in Equation \eqref{triplet:single:loss} using all the triplets in set.

\begin{ndef}
Given a set of triplets, each containing an anchor, a positive example and a negative example, $\mathcal T = \{(x_i,\ps_i,\ns_i)\}_{i \in \Lambda}$, we define a triplet loss as follows:
\begin{equation}\label{triplet:sum:loss}
\mathcal L (x_i,\ps_i,\ns_i) = \sum_{i \in \Lambda} \ell^\alpha(x_i,\ps_i,\ns_i).
\end{equation}

\end{ndef}



We use this loss to train models in order to improve the representations obtained. It would be interesting to present the model non-trivial metric to the learning algorithm. When the representation $g$ improves, this is harder to do, and this results in slow convergence and expensive data sampling methods.

\section{Generalization of triplet losses and $N-$pairs loss}

In a single evaluation of the loss function over a triplet during the learning process, we are comparing one positive sample to one negative sample. In practice, after looping over sufficiently many triplets, we expect the distance between positive examples and negative examples to be maximized. However, this will surely be a slow process if our dataset has many examples and also, in each step we will be separating the positive element from the specific negative element to which we are comparing it in that evaluation. Thus, the technique might be unstable \citep{Sohn2016ImprovedDM}.

In order to fix this, a good idea would be to compare in each evaluation a positive sample with multiple negative samples, generalizing the case exposed before. This way, we would like the positive sample to increase its distance to \emph{all} of the negative samples at the same time. Let us present a loss that generalizes the loss in Equation \eqref{triplet:sum:loss}.

\begin{ndef}
Let $\ps$ be a positive example of the anchor $x$, and consider the set $X^- = \{\ns_1,\cdots,\ns_{N-1}\}$ of $(N-1)$ negative samples. Given an encoder $g$, the $(N+1)-$tuplet loss is defined as follows:
\begin{equation}\label{nplus1:tuplet:loss}
\mathcal L_{(N+1)-\text{tuplet}}(x,\ps,X^-) = \log \left( 1+ \sum_{i=1}^{N-1} \exp \left(g(x)^T g(\ns_i) - g(x)^T g(\ps)\right)\right) 
\end{equation}
\end{ndef}

\begin{remark}
If we consider the case $N=2$, we have
\[
\mathcal L_{(2+1)-\text{tuplet}}(x,\ps,\ns) = \log \left( 1+ \exp \left(g(x)^Tg(\ns) - g(x)^T g(\ps)\right)\right).
\]
This expression is very similar to the one in Equation \eqref{triplet:single:loss}. In fact, if the norm in Equation \eqref{triplet:single:loss} is unit and $g$ minimizes $\mathcal L_{(2+1)-\text{tuplet}}$, then it minimizes $\ell^\alpha$, and hence both losses are equivalent.
\end{remark}



Applying the $(N+1)-$tuplet loss in deep metric learning is computationally expensive. Indeed, if we apply Stochastic Gradient Descent (SGD) with batch size $M$, then we have to evaluate $M \times (N+1)$ times our function $\ell^\alpha$ in each update. Because of this, if we increase $M$ and $N$, the number of evaluations grows quadratically. We would like to avoid this.

Consider the set of $N$ \emph{pairs} of examples, with the constraint of each pair belonging to a different class, i.e. $X = \{(x_1,\ps_1),\cdots,(x_N,\ps_N)\}$ with $y_i \neq y_j$ for all $i \neq j$. We now build $N$ tuplets where each tuplet has all the positive samples and the $i-th$ anchor, that is:
\[
\{S_i\}_{i=1}^N, \quad \text{where} \quad S_i = \{x_i, \ps_1,\cdots,\ps_N\}.  
\]
We can consider that each tuplet has $x_i$ as anchor, $\ps_i$ as positive example and $\ps_j$ for $j \neq i$ as negative samples, since they were all from different classes.

\begin{ndef}
In the last conditions, we can define the \emph{multi class N-pair loss} as follows:
\begin{align}
\mathcal L_{N-pair-mc} & \left(\left\{(x_i,\ps_i)\right\}_{i=1}^N \right)  = \nonumber \\
& \frac{1}{N} \sum_{i=1}^N \log \left(1+ \sum_{j\neq i} \exp\left(g(x_i)^T g(\ps_j) - g(x_i)^Tg(\ps_i)\right)\right) \label{N:Pair:loss}
\end{align}
\end{ndef}
This way, we are combining the $(N+1)-$tuplet loss and the $N-$pair construction that we presented, enabling highly scalable training. This loss has empirically proved in to have advantages if we compare it to other variations of mini-batch methods \citep{Sohn2016ImprovedDM}.

\subsection{InfoNCE Bound as a triplet loss}

The InfoNCE loss on Equation \eqref{NCE:loss} has proved to be useful in representation learning. Let us consider a reformulation on it. Firstly, since $f_k$ was an exponential, we can also consider $e^f$ and remove the exponential from $f$, this is just notation. Now, in \cite{poole_variational_2019} the InfoNCE bound in \eqref{Bound:NCE} is rewritten as follows:
\[
I(X,Y) \geq E\left[ \frac{1}{N} \sum_{i = 1}^N \log \frac{e^{f(x_i,y_i)}}{\frac{1}{N}\sum_{j=1}^N e^{f(x_i,y_j)}}\right] \triangleq  I_{NCE}(X,Y)
\]
where we have just named the right hand side of the inequality as $I_{NCE}(X,Y)$. Now, we can transform it in the following way:
\begin{align}
I_{NCE}  & = E\left[ \frac{1}{N} \sum_{i = 1}^N \log \frac{e^{f(x_i,y_i)}}{\frac{1}{N}\sum_{j=1}^N e^{f(x_i,y_j)}}\right]\nonumber \\
& = E\left[ \frac{1}{N} \sum_{i = 1}^N \log \frac{1}{\frac{1}{N}\sum_{j=1}^N e^{f(x_i,y_j) - f(x_i,y_i)}}\right]\nonumber \\
& = E\left[ -  \frac{1}{N} \sum_{i = 1}^N \log \frac{1}{N} \sum_{j=1}^N e^{f(x_i,y_j) - f(x_i,y_i)}\right] \label{last:eq:INCE}
\end{align}
And now, we only have to use the basic properties of the logarithm to see that
\begin{align}
\text{\eqref{last:eq:INCE}} & = E\left[-\frac{1}{N}\left(\sum_{i=1}^N \log \frac{1}{N} + \sum_{i=1}^N \log \sum_{j = 1}^N e^{f(x_i,y_j) - f(x_i,y_i)} \right)\right] \nonumber \\
& = E\left[-\frac{1}{N}\left( N(\cancel{\log 1} - \log N ) + \sum_{i=1}^N \log \left( 1  + \sum_{j \neq i} e^{f(x_i,y_j) - f(x_i,y_i)} \right)\right)\right] \nonumber\\ 
& = E\left[-\frac{1}{N}\left(-N\log N\right)\right] + E\left[-\frac{1}{N}\sum_{i=1}^N \log \left( 1  + \sum_{j \neq i} e^{f(x_i,y_j) - f(x_i,y_i)} \right)\right] \nonumber \\
& = \log N - E\left[ \frac{1}{N} \sum_{i=1}^N \log \left( 1+ \sum_{j\neq i}e^{f(x_i,y_j)- f(x_i,y_i)}\right)\right] \label{final:form:INCE}
\end{align}

In the particular case where $X,Y$ take values in the same space and $f$ has the particular form 
\[
f(x,y) = \phi(x)^T \phi(y),  
\]
for some $\phi$, Equation \eqref{final:form:INCE} is the same (up to constants and change of sign) as the expectation of the multi-class $N-$pair loss in Equation \eqref{N:Pair:loss}.

Hence, we have found an equivalence between representation learning by maximizing $I_{NCE}$ using a symmetric separable critic $f(x,y)$ and an encoder $g$ shared across views and using a multi class $N-$pair loss, which is a generalization of a triplet loss.


